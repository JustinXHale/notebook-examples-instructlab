{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af99f876-0ffd-4079-aeb7-4cead05daaf4",
   "metadata": {},
   "source": [
    "# 🐶 Data Pre-Processing: From source PDF to SDG-ready\n",
    "\n",
    "This notebook goes through each of the stages of data pre-processing. Directory-based conventions are used to save intermediate results as a PDF is converted and chunked and QA generation is performed to create a `qna.yaml` file for each knowledge contribution. At the end everything is combined into the inputs for SDG.\n",
    "\n",
    "Once a SDG seed dataset is created, a user can run through an SDG notebook and generate samples.\n",
    "\n",
    "**NOTE**: Starting the notebook using Python 3.11 is recommended. Python 3.12 or later are not yet supported. \n",
    "\n",
    "1. [Data Gathering](#Data-Gathering)\n",
    "1. [Document Conversion](#Document-Conversion)\n",
    "1. [Chunking](#Chunking)\n",
    "1. [Authoring](#Authoring)\n",
    "1. [Create Seed Dataset](#Create-Seed-Dataset-for-SDG)\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f76bec",
   "metadata": {},
   "source": [
    "TODO: Document how to organize directory structure, etc\n",
    "\n",
    "Store your source documents under: `workspaces > workspace_name > contribution name > source documents`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0acd026f-65bd-4393-bb40-f8aa8bd6828b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "WORKSPACE_NAME = \"default\"\n",
    "\n",
    "WORKSPACE_ROOT = Path(\"workspaces\")\n",
    "WORKSPACE_ROOT.mkdir(exist_ok=True)\n",
    "\n",
    "WORKSPACE_DIR = WORKSPACE_ROOT / WORKSPACE_NAME\n",
    "WORKSPACE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "contribution_dirs = []\n",
    "# contribution_names = [\"nfl\"]  # ADD CONTRIBUTION NAMES HERE\n",
    "contribution_names = [\"nfl\", \"finance\"]\n",
    "contribution_metadata = [{\"domain\": \"sports\", \"summary\": \"Official playing rules of the National Football League 2022\"}, {\"domain\": \"finance\", \"summary\": \"Account information for a specific bank\"}]\n",
    "\n",
    "\n",
    "for name in contribution_names:\n",
    "    contribution_dir = WORKSPACE_DIR / name\n",
    "    contribution_dirs.append(contribution_dir)\n",
    "\n",
    "    for subdir in [\"source_documents\", \"conversion\", \"chunking\", \"authoring\"]:\n",
    "        (contribution_dir / subdir).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68478061",
   "metadata": {},
   "source": [
    "TODO\n",
    "\n",
    "Now that the directory structure is created, ensure your source documents are placed under the appropriate `<contribution_name>/source_documents` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344b7ac5-fc2a-40a8-8e1f-e8dd8b1153e7",
   "metadata": {},
   "source": [
    "## Data Gathering\n",
    "\n",
    "TODO: Add documentation about domain and summary here, clear out second contribution example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26501e2f-7215-441f-9efa-075f87024893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files to convert:\n",
      "/Users/amaredia/dev/examples/notebooks/instructlab-knowledge/workspaces/default/nfl/source_documents/2022-nfl-rulebook-final.pdf\n",
      "/Users/amaredia/dev/examples/notebooks/instructlab-knowledge/workspaces/default/finance/source_documents/BofA_InterestChecking_en_ADA.pdf\n",
      "/Users/amaredia/dev/examples/notebooks/instructlab-knowledge/workspaces/default/finance/source_documents/Advantag Savings.pdf\n"
     ]
    }
   ],
   "source": [
    "print(f\"Files to convert:\")\n",
    "for dir in contribution_dirs:\n",
    "    files = list((dir / \"source_documents\").glob(\"*.pdf\"))\n",
    "    for file in files:\n",
    "        print(file.resolve())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4904e6-8e12-4473-8301-cba90e61bd8b",
   "metadata": {},
   "source": [
    "## Document Conversion\n",
    "\n",
    "This notebook uses [Docling](https://github.com/docling-project/docling) to convert any type of document into a Docling Document. A Docling Document is the representation of the document after conversion that can be exported as JSON. The JSON output of this notebook can then be used in others such as one that uses Docling's chunking methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b91d4b2e-19cd-46e7-a912-ba9b2904c7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq docling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749fb64b-d089-4844-9330-7f3639819e7a",
   "metadata": {},
   "source": [
    "### Configure Docling conversion pipeline\n",
    "\n",
    "Next we set the configuration options for our conversion pipeline. The PDF Conversion options set here are the defaults. More information about pipeline configuration can be found on Docling.\n",
    "\n",
    "For a complete reference on Docling conversion pipeline configuration, see [PDFPipelineOptions](https://docling-project.github.io/docling/reference/pipeline_options/#docling.datamodel.pipeline_options.PdfPipelineOptions) and [PDFFormatOptions](https://docling-project.github.io/docling/reference/document_converter/#docling.document_converter.InputFormat.XML_JATS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "157c5e02-edd1-44f6-b20f-f6b4bda1aae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
    "\n",
    "pipeline_options = PdfPipelineOptions() # TODO: show the options that can be set\n",
    "\n",
    "doc_converter = DocumentConverter(\n",
    "    format_options={\n",
    "        InputFormat.PDF: PdfFormatOption(\n",
    "            pipeline_options=pipeline_options\n",
    "        )\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73400c74-dead-4998-aee2-ddb00ddaa276",
   "metadata": {},
   "source": [
    "Finally, we convert every document into Docling JSON as long as it is a valid file type to be converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a200039c-b8b2-4087-88ba-7bfb0e393cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PosixPath('workspaces/default/nfl/source_documents/2022-nfl-rulebook-final.pdf')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amaredia/dev/examples/notebooks/instructlab-knowledge/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path of JSON output is: /Users/amaredia/dev/examples/notebooks/instructlab-knowledge/workspaces/default/nfl/conversion/2022-nfl-rulebook-final.json\n",
      "[PosixPath('workspaces/default/finance/source_documents/BofA_InterestChecking_en_ADA.pdf'), PosixPath('workspaces/default/finance/source_documents/Advantag Savings.pdf')]\n",
      "Path of JSON output is: /Users/amaredia/dev/examples/notebooks/instructlab-knowledge/workspaces/default/finance/conversion/BofA_InterestChecking_en_ADA.json\n",
      "Path of JSON output is: /Users/amaredia/dev/examples/notebooks/instructlab-knowledge/workspaces/default/finance/conversion/Advantag Savings.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json_files=[]\n",
    "for contribution_dir in contribution_dirs:\n",
    "    files = list((contribution_dir / \"source_documents\").glob(\"*.pdf\"))\n",
    "    \n",
    "    for file in files:\n",
    "        doc = doc_converter.convert(source=file).document\n",
    "        doc_dict = doc.export_to_dict()\n",
    "   \n",
    "        conversion_output_dir = contribution_dir / \"conversion\"\n",
    "        conversion_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        json_output_path = conversion_output_dir / f\"{file.stem}.json\"\n",
    "        with open(json_output_path, \"w\") as f:\n",
    "            json.dump(doc_dict, f)\n",
    "            print(f\"Path of JSON output is: {Path(json_output_path).resolve()}\")\n",
    "            json_files.append(json_output_path.resolve())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40710019-7ec9-414e-ad72-1ba672cf5fc2",
   "metadata": {},
   "source": [
    "## Post-Conversion: Illuminator Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2572e2d0-94dc-4ca0-b032-3978af26c9c9",
   "metadata": {},
   "source": [
    "The output of document conversion is not always perfect. Data may become distorted or corrupted, which can negatively affect a model's performance after training. While optional, reviewing your converted data is strongly recommended. The following example explains how to use the Illuminator tool to identify common conversion issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e07e35-befb-4ed5-9fe4-41544f88d943",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.illuminator.analysis import analyze_docling_tables\n",
    "from utils.illuminator.utils import generate_summary\n",
    "from docling.datamodel.document import DoclingDocument\n",
    "\n",
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "results = {}\n",
    "\n",
    "for path in converted_json_paths:\n",
    "    with open(path, \"r\") as f:\n",
    "        doc_dict = json.load(f)\n",
    "\n",
    "    doc = DoclingDocument(**doc_dict)\n",
    "    results[path] = analyze_docling_tables(doc)\n",
    "\n",
    "summary_path = Path(\"illuminator_readable_summary.txt\")\n",
    "\n",
    "with open(summary_path, \"w\") as f:\n",
    "    generate_summary(results, file=f)\n",
    "\n",
    "print(f\"✅ Post-conversion summary saved to: {summary_path.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea0876e-ac55-45fc-93e8-3e646a6c3104",
   "metadata": {},
   "source": [
    "\n",
    "The output of this post-conversion step should help determine whether to avoid using the content for chunking entirely or to manually edit it before proceeding with chunking.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafad55e-a4c0-4d6e-9da0-49519fa9bf74",
   "metadata": {},
   "source": [
    "## Chunking\n",
    "\n",
    "The goal of chunking the converted documents is to provide the teacher model small and logical pieces of the source document to generate data off of.\n",
    "\n",
    "In this notebook we are doing chunking with [Docling](https://docling-project.github.io/docling/examples/hybrid_chunking/#hybrid-chunking).\n",
    "\n",
    "The input to this notebook is a docling JSON file created after a docling conversion, or a directory of docling JSON files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2482060c-a49f-4345-aa47-d54301939387",
   "metadata": {},
   "source": [
    "### Initialize the Chunker\n",
    "\n",
    "Docling provides two chunkers, the `HierarchicalChunker` and the `HybridChunker`.\n",
    "The `HierarchicalChunker` creates chunks based on the hierarchy in the Docling document\n",
    "\n",
    "The `HybridChunker` builds on the `HierarchicalChunker` and by making it tokenization aware.\n",
    "\n",
    "The `HybridChunker` has options for a `tokenizer`, the `max_tokens` in a chunk, and whether to merge undersized peer chunks. Uncomment the commented out code to configure these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50df9d91-add4-46a1-a69d-0f7f9f69542e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from docling_core.transforms.chunker.tokenizer.huggingface import HuggingFaceTokenizer\n",
    "#from transformers import AutoTokenizer\n",
    "\n",
    "from docling.chunking import HybridChunker\n",
    "\n",
    "#EMBED_MODEL_ID = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "#MAX_TOKENS = 1024\n",
    "#\n",
    "# tokenizer = HuggingFaceTokenizer(\n",
    "#     tokenizer=AutoTokenizer.from_pretrained(EMBED_MODEL_ID),\n",
    "#     max_tokens=MAX_TOKENS,  # optional, by default derived from `tokenizer` for HF case\n",
    "#     merge_peers=True # \n",
    "# )\n",
    "\n",
    "chunker = HybridChunker(\n",
    "    #tokenizer=tokenizer,\n",
    "    #merge_peers=True,  # whether to merge undersized chunks - defaults to True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ce1d6f-b8d3-470c-b3c9-675911f0ee92",
   "metadata": {},
   "source": [
    "### Load and chunk the converted docling document\n",
    "\n",
    "Next lets convert the document we want to chunk up into a Docling Document.\n",
    "\n",
    "All chunks are saved to a JSON file called chunks.jsonl in the `chunks` directory in your contribution. This file is one of the inputs father below when we create the seed dataset for SDG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "db983c05-4aa6-4261-9283-2adab69bfbd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 1798 chunks from 2022-nfl-rulebook-final\n",
      "Path of chunks JSON is: /Users/amaredia/dev/examples/notebooks/instructlab-knowledge/workspaces/default/nfl/chunks/chunks.jsonl\n",
      "Extracted 5 chunks from Advantag Savings\n",
      "Path of chunks JSON is: /Users/amaredia/dev/examples/notebooks/instructlab-knowledge/workspaces/default/finance/chunks/chunks.jsonl\n",
      "Extracted 10 chunks from BofA_InterestChecking_en_ADA\n",
      "Path of chunks JSON is: /Users/amaredia/dev/examples/notebooks/instructlab-knowledge/workspaces/default/finance/chunks/chunks.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "for contribution_dir in contribution_dirs:\n",
    "    conversion_dir = contribution_dir / \"conversion\"\n",
    "    json_files = list(conversion_dir.glob(\"*.json\"))\n",
    "    chunking_output_dir = contribution_dir / \"chunks\"\n",
    "    chunking_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    all_chunks = []\n",
    "    \n",
    "    for file in json_files:\n",
    "        # reconvert the docling JSON for chunking\n",
    "        doc = DocumentConverter().convert(source=file)\n",
    "        \n",
    "        chunk_iter = chunker.chunk(dl_doc=doc.document)\n",
    "        chunk_objs = list(chunk_iter)\n",
    "    \n",
    "        print(f\"Extracted {len(chunk_objs)} chunks from {doc.document.name}\")\n",
    "        \n",
    "        for chunk in chunk_objs:\n",
    "            c = dict(chunk=chunker.contextualize(chunk=chunk), file=doc.document.name,metadata=chunk.meta.export_json_dict())\n",
    "            all_chunks.append(c)\n",
    "\n",
    "        chunks_file_path = chunking_output_dir / \"chunks.jsonl\"\n",
    "        with open(chunks_file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            for chunk in all_chunks:\n",
    "                json.dump(chunk, file)\n",
    "                file.write(\"\\n\")\n",
    "            print(f\"Path of chunks JSON is: {Path(chunks_file_path).resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb38545-eb84-4923-8fc4-d10ed08eab26",
   "metadata": {},
   "source": [
    "### View the Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4fdf34c7-9829-43d2-bf9f-7d1d55bb6a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_gen = iter(all_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811992ac",
   "metadata": {},
   "source": [
    "To view the chunks one by one, rerun the following cell. As you can see the document is broken into small pieces with metadata about the chunk based on the document's format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee9a8531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interest rate policy\n",
      "Interest rate and interest calculation\n",
      "Your account has a variable interest rate, which means the interest rate may change. Interest rates for your account are based on balance tiers, are set at our discretion and may change at any time without notice.\n",
      "To calculate interest, we apply a daily periodic rate to the collected balance in your account each day. The daily rate that applies will depend on which balance tier your end-of-day balance falls in. We compound and pay any interest earned monthly. Interest is only paid in whole cents and we use standard rounding rules to calculate the amount. This means that an amount less than half of one cent is rounded down to zero, and an amount of half of one cent or more is rounded up to the next whole cent.\n",
      "Extra interest\n",
      "Extra interest is earned automatically when an account owner is enrolled in the Preferred Rewards program. The interest rate will increase by at least 5%, 10% or 20% over the standard rate, and is based on the Preferred Rewards tier in which you're enrolled. We may decide to pay a higher rate.\n",
      "Where can I find information about current interest rates?\n",
      "You can find current rate information by checking bankofamerica.com, calling the number on your account statement or visiting a financial center.\n"
     ]
    }
   ],
   "source": [
    "print(next(chunk_gen)['chunk'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a510f8c7-8cd3-4867-8742-9f4f9cda9e9f",
   "metadata": {},
   "source": [
    "## Authoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3490c8a-5ee8-44cd-ae5e-26a6ca7b4017",
   "metadata": {},
   "source": [
    "### Install docling-sdg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86c48e52-cda7-48ac-84dc-0b844aed5f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq docling-sdg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65ec755-e3de-40ab-bf3a-23ebb29a705d",
   "metadata": {},
   "source": [
    "### Initialize QA generator model & Number of Seed examples\n",
    "\n",
    "To generate the Open AI compatible endpoint, API key, and model name for the as well as the number of seed example you wish to generate for your contribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "874d4de8",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "API_KEY = \"bc4f8ec17edae8eb015cb76a8cca6363\"  # the API access key for your account ( cannot be empty )\n",
    "API_URL = \"https://mixtral-8x7b-instruct-v0-1-maas-apicast-production.apps.prod.rhoai.rh-aiservices-bu.com/v1\"  # the URL of your model's API\n",
    "MODEL_ID = \"mistralai/Mixtral-8x7B-Instruct-v0.1\" # the name of your model\n",
    "NUM_SEED_EXAMPLES = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b702267e-f550-4bc2-bce4-c0fcecbbd292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering smaller chunks out of document 2022-nfl-rulebook-final\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:16<00:00,  3.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status for Q&A generation for nfl is: Status.SUCCESS\n",
      "qna.yaml saved to: workspaces/default/nfl/authoring/qna.yaml\n",
      "Filtering smaller chunks out of document Advantag Savings\n",
      "Filtering smaller chunks out of document BofA_InterestChecking_en_ADA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:17<00:00,  3.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status for Q&A generation for finance is: Status.SUCCESS\n",
      "qna.yaml saved to: workspaces/default/finance/authoring/qna.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from utils.qna_gen import generate_seed_examples\n",
    "\n",
    "for contribution_name, contribution_dir, contribution_metadata in zip(contribution_names, contribution_dirs, contribution_metadata):\n",
    "    qna_output_path = generate_seed_examples(contribution_name,\n",
    "                           contribution_dir,\n",
    "                           contribution_metadata,\n",
    "                           NUM_SEED_EXAMPLES,\n",
    "                           API_KEY,\n",
    "                           API_URL,\n",
    "                           MODEL_ID)\n",
    "    print(f\"qna.yaml saved to: {qna_output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c574f96-5860-48b9-b4ac-01d367c7717b",
   "metadata": {},
   "source": [
    "### Revise QAs\n",
    "\n",
    "Open the generated `qna.yaml` in your preferred text editor to ensure the quality of generated questions and answers. If the generation step has failed to generated three questions and answers for each of five contexts, supplant until that required number of QA pairs is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f101076-a50f-49ea-a83b-46eaa8b39cc4",
   "metadata": {},
   "source": [
    "## Create Seed Dataset for SDG\n",
    "\n",
    "This section combines the contents from the qna.yaml and the chunks from the source document to create a seed dataset for the synthetic data generation process.\n",
    "\n",
    "To run this step you need a directory that contains `chunks.jsonl` and a `qna.yaml` in the same directory.\n",
    "\n",
    "This step outputs a seed.jsonl file in the SDG_OUTPUT_DIR that you set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2c6e31b-e8a9-406c-b2dc-27433c8fd8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq datasets transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab2c9ed2-8ba8-4959-8e01-81625b81d286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33d19f33384244ed9294d7b8c8ed6fb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1798 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fe5dc6cd62e4bf5a112cb8f960da2cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1798 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca66ef9ca6ef4472931b409a7a58acea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1798 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe18fa1a49b44c9ab5ab8bd933dc63a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1798 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b73456556d64f52abf4091581bc1d2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1798 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25cf6b84b7c54feaa602c177035d2c06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8990 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f06f5bbb3bbd453dbd26046754f2500b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/8990 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18ed2ba0a5a247cabf4a1598c1533f09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/8 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate results saved to: workspaces/default/nfl/seed_data-nfl.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36e7486560fc4fc1af7d8f7159a77233",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a54d79cb957c44f88d1e85207dfec611",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b161f41a93d44efb5558a7bf5a47b27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15e563424ff64dab98151806b0548a00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3fd16612b524953a2dee9a08b9c23c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f9b3dc413bb499393784d9502cec874",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b24febbdf4a44434b744ab27e7c52d19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/25 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b3960a631364ce296f1b1323f936314",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21948d07989e444997be0f1d30e28349",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1920f6b58d543bba27255212aad1e7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e83ceeca69534d068bf6263c24d9459f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96bd7c28f5cd494bb1cd29e6a35815f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b06a4501a33249e093111f754b1e5b7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b13a17b92db47609549ae1d48eca645",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46ea135794784f98a9025f59c66211b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate results saved to: workspaces/default/finance/seed_data-finance.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9af2ab1d557b4018a838c9f75ff4ce68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/8 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed data contains 9065 rows\n",
      "Final results saved to: workspaces/default/seed_data.jsonl\n"
     ]
    }
   ],
   "source": [
    "from utils.create_seed_dataset import get_seed_dataset, safe_concatenate_datasets\n",
    "\n",
    "contribution_datasets = []\n",
    "for contribution_dir, contribution_name in zip(contribution_dirs, contribution_names):\n",
    "    chunks_dir = contribution_dir / \"chunks\"\n",
    "    qna_dir = contribution_dir / \"authoring\"\n",
    "    seed_data = get_seed_dataset(chunks_dir, qna_dir)\n",
    "    output_path = f'{contribution_dir}/seed_data-{contribution_name}.jsonl'\n",
    "    seed_data.to_json(output_path, orient='records', lines=True)\n",
    "    contribution_datasets.append(seed_data)\n",
    "    print(f\"Intermediate results saved to: {output_path}\")\n",
    "\n",
    "final_seed_data = safe_concatenate_datasets(contribution_datasets)\n",
    "output_path = f'{WORKSPACE_DIR}/seed_data.jsonl'\n",
    "final_seed_data.to_json(output_path, orient='records', lines=True)\n",
    "\n",
    "print(f\"Final seed data contains {final_seed_data.data.num_rows} rows\")\n",
    "print(f\"Final seed data for SDG saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ff36f4-19fc-4a27-b51a-3688e7b630e4",
   "metadata": {},
   "source": [
    "### Inspect the seed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6936825-31c1-4b46-a1af-2fb46f50158d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(seed_data.data.table.slice(length=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a8fcdb-8035-4f30-b856-46afe9f928a1",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "To recap, given a source document in PDF format, this notebook:\n",
    "\n",
    "1. Converted the document using document and saved it to JSON for inspection\n",
    "2. Split the extracted text into chunks\n",
    "3. Generated QA pairs for a subset of those chunks\n",
    "4. Created a `qna.yaml` available for inspection and revision\n",
    "5. Combined the chunks and `qna.yaml` to create a `seed_data.jsonl` for use with SDG\n",
    "\n",
    "The next step is to use the resulting `seed_data.jsonl` for SDG, such as illustrated in [this notebook](https://github.com/Red-Hat-AI-Innovation-Team/sdg_hub/blob/main/examples/instructlab/knowledge/knowledge_generation_and_mixing.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
