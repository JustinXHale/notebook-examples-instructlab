{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af99f876-0ffd-4079-aeb7-4cead05daaf4",
   "metadata": {},
   "source": [
    "# 🐶 Data Pre-Processing: From source PDF to SDG-ready\n",
    "\n",
    "This notebook goes through each of the stages of data pre-processing. Directory-based conventions are used to save intermediate results as a PDF is converted and chunked and QA generation is performed to create a `qna.yaml` file for each knowledge contribution. At the end everything is combined into the inputs for SDG.\n",
    "\n",
    "Once a SDG seed dataset is created, a user can run through an SDG notebook and generate samples.\n",
    "\n",
    "**NOTE**: Starting the notebook using Python 3.11 is recommended. Python 3.12 or later are not yet supported. \n",
    "\n",
    "1. [Data Gathering](#Data-Gathering)\n",
    "1. [Document Conversion](#Document-Conversion)\n",
    "1. [Chunking](#Chunking)\n",
    "1. [Authoring](#Authoring)\n",
    "1. [Create Seed Dataset](#Create-Seed-Dataset-for-SDG)\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f76bec",
   "metadata": {},
   "source": [
    "TODO: Document how to organize directory structure, etc\n",
    "\n",
    "Store your source documents under: `workspaces > workspace_name > contribution name > source documents`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0acd026f-65bd-4393-bb40-f8aa8bd6828b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "WORKSPACE_NAME = \"default\"\n",
    "\n",
    "WORKSPACE_ROOT = Path(\"workspaces\")\n",
    "WORKSPACE_ROOT.mkdir(exist_ok=True)\n",
    "\n",
    "WORKSPACE_DIR = WORKSPACE_ROOT / WORKSPACE_NAME\n",
    "WORKSPACE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "contribution_dirs = []\n",
    "contribution_names = [\"nfl\"]  # ADD CONTRIBUTION NAMES HERE\n",
    "# contribution_names = [\"nfl\", \"finance\"]\n",
    "\n",
    "# SOURCE_DOCUMENT_DIR = WORKSPACE_DIR / \"source_documents\"\n",
    "# SOURCE_DOCUMENT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for name in contribution_names:\n",
    "    contribution_dir = WORKSPACE_DIR / name\n",
    "    contribution_dirs.append(contribution_dir)\n",
    "\n",
    "    for subdir in [\"source_documents\", \"conversion\", \"chunking\", \"authoring\"]:\n",
    "        (contribution_dir / subdir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# CONVERSION_OUTPUT_DIR = WORKSPACE_DIR / \"conversion\"\n",
    "# CONVERSION_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# CHUNKING_OUTPUT_DIR = WORKSPACE_DIR / \"chunking\"\n",
    "# CHUNKING_OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# AUTHORING_OUTPUT_DIR = WORKSPACE_DIR / \"authoring\"\n",
    "# AUTHORING_OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# SDG_OUTPUT_DIR = WORKSPACE_DIR / \"sdg\"\n",
    "# SDG_OUTPUT_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68478061",
   "metadata": {},
   "source": [
    "TODO\n",
    "\n",
    "Now that the directory structure is created, ensure your source documents are placed under the appropriate `<contribution_name>/source_documents` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344b7ac5-fc2a-40a8-8e1f-e8dd8b1153e7",
   "metadata": {},
   "source": [
    "## Data Gathering\n",
    "\n",
    "TODO: Add documentation about domain and summary here, clear out second contribution example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26501e2f-7215-441f-9efa-075f87024893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files to convert:\n",
      "[PosixPath('workspaces/demo1/nfl/source_documents/2022-nfl-rulebook-final.pdf')]\n"
     ]
    }
   ],
   "source": [
    "# for contribution_dir in contribution_dirs:\n",
    "#     source_documents_dir = contribution_dir / \"source_documents\"\n",
    "#     source_documents_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# struct would have document outline, domain, and path to pdf's for each contribution\n",
    "\n",
    "contribution_path = Path(WORKSPACE_DIR / \"nfl\" / \"source_documents\")\n",
    "# contribution_prefix = \"nfl\"\n",
    "contribution_domain = \"sports\" \n",
    "contribution_summary = \"Official playing rules of the National Football League 2022\"\n",
    "\n",
    "# contribution1 = {\"path\": contribution_path, \"prefix\": contribution_prefix, \"domain\": contribution_domain, \"summary\": contribution_summary}\n",
    "contribution1 = {\"path\": contribution_path, \"domain\": contribution_domain, \"summary\": contribution_summary}\n",
    "\n",
    "# contribution_path2 = Path(SOURCE_DOCUMENT_DIR / \"finance\")\n",
    "# contribution_prefix2 = \"finance\"\n",
    "# contribution_domain2 = \"banking\" \n",
    "# contribution_summary2 = \"Account information for a specific bank\"\n",
    "\n",
    "# contribution2 = {\"path\": contribution_path2, \"prefix\": contribution_prefix2, \"domain\": contribution_domain2, \"summary\": contribution_summary2}\n",
    "\n",
    "contributions = []\n",
    "contributions.append(contribution1)\n",
    "# contributions.append(contribution2)\n",
    "\n",
    "for contribution in contributions:\n",
    "    contribution[\"files\"] = list(contribution[\"path\"].glob(\"*.pdf\"))\n",
    "\n",
    "print(f\"Files to convert:\")\n",
    "for contribution in contributions:\n",
    "    print(contribution['files'])\n",
    "    # conv_output_dir = CONVERSION_OUTPUT_DIR / contribution[\"prefix\"]\n",
    "    # conv_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # chunking_output_dir = CHUNKING_OUTPUT_DIR / contribution[\"prefix\"]\n",
    "    # chunking_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # authoring_output_dir = AUTHORING_OUTPUT_DIR / contribution[\"prefix\"]\n",
    "    # authoring_output_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4904e6-8e12-4473-8301-cba90e61bd8b",
   "metadata": {},
   "source": [
    "## Document Conversion\n",
    "\n",
    "This notebook uses [Docling](https://github.com/docling-project/docling) to convert any type of document into a Docling Document. A Docling Document is the representation of the document after conversion that can be exported as JSON. The JSON output of this notebook can then be used in others such as one that uses Docling's chunking methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b91d4b2e-19cd-46e7-a912-ba9b2904c7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq docling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749fb64b-d089-4844-9330-7f3639819e7a",
   "metadata": {},
   "source": [
    "### Configure Docling conversion pipeline\n",
    "\n",
    "Next we set the configuration options for our conversion pipeline. The PDF Conversion options set here are the defaults. More information about pipeline configuration can be found on Docling.\n",
    "\n",
    "For a complete reference on Docling conversion pipeline configuration, see [PDFPipelineOptions](https://docling-project.github.io/docling/reference/pipeline_options/#docling.datamodel.pipeline_options.PdfPipelineOptions) and [PDFFormatOptions](https://docling-project.github.io/docling/reference/document_converter/#docling.document_converter.InputFormat.XML_JATS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "157c5e02-edd1-44f6-b20f-f6b4bda1aae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ksulayma/repos/ilab/examples/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
    "\n",
    "pipeline_options = PdfPipelineOptions() # TODO: show the options that can be set\n",
    "\n",
    "doc_converter = DocumentConverter(\n",
    "    format_options={\n",
    "        InputFormat.PDF: PdfFormatOption(\n",
    "            pipeline_options=pipeline_options\n",
    "        )\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73400c74-dead-4998-aee2-ddb00ddaa276",
   "metadata": {},
   "source": [
    "Finally, we convert every document into Docling JSON as long as it is a valid file type to be converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a200039c-b8b2-4087-88ba-7bfb0e393cc9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'contribution_dirs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m      3\u001b[39m json_files=[]\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m contribution_dir \u001b[38;5;129;01min\u001b[39;00m \u001b[43mcontribution_dirs\u001b[49m:\n\u001b[32m      5\u001b[39m     files = \u001b[38;5;28mlist\u001b[39m((contribution_dir / \u001b[33m\"\u001b[39m\u001b[33msource_documents\u001b[39m\u001b[33m\"\u001b[39m).glob(\u001b[33m\"\u001b[39m\u001b[33m*.pdf\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m      6\u001b[39m     \u001b[38;5;28mprint\u001b[39m(files)\n",
      "\u001b[31mNameError\u001b[39m: name 'contribution_dirs' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json_files=[]\n",
    "for contribution_dir in contribution_dirs:\n",
    "    files = list((contribution_dir / \"source_documents\").glob(\"*.pdf\"))\n",
    "    print(files)\n",
    "    \n",
    "    for file in files:\n",
    "        doc = doc_converter.convert(source=file).document\n",
    "        doc_dict = doc.export_to_dict()\n",
    "   \n",
    "        conversion_output_dir = contribution_dir / \"conversion\"\n",
    "        conversion_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        json_output_path = conversion_output_dir / f\"{file.stem}.json\"\n",
    "        with open(json_output_path, \"w\") as f:\n",
    "            json.dump(doc_dict, f)\n",
    "            print(f\"Path of JSON output is: {Path(json_output_path).resolve()}\")\n",
    "            json_files.append(json_output_path.resolve())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40710019-7ec9-414e-ad72-1ba672cf5fc2",
   "metadata": {},
   "source": [
    "## Post-Conversion: Illuminator Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2572e2d0-94dc-4ca0-b032-3978af26c9c9",
   "metadata": {},
   "source": [
    "The output of document conversion is not always perfect. Data may become distorted or corrupted, which can negatively affect a model's performance after training. While optional, reviewing your converted data is strongly recommended. The following example explains how to use the Illuminator tool to identify common conversion issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e07e35-befb-4ed5-9fe4-41544f88d943",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.illuminator.analysis import analyze_docling_tables\n",
    "from utils.illuminator.utils import generate_summary\n",
    "from docling.datamodel.document import DoclingDocument\n",
    "\n",
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "results = {}\n",
    "\n",
    "for path in converted_json_paths:\n",
    "    with open(path, \"r\") as f:\n",
    "        doc_dict = json.load(f)\n",
    "\n",
    "    doc = DoclingDocument(**doc_dict)\n",
    "    results[path] = analyze_docling_tables(doc)\n",
    "\n",
    "summary_path = Path(\"illuminator_readable_summary.txt\")\n",
    "\n",
    "with open(summary_path, \"w\") as f:\n",
    "    generate_summary(results, file=f)\n",
    "\n",
    "print(f\"✅ Post-conversion summary saved to: {summary_path.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea0876e-ac55-45fc-93e8-3e646a6c3104",
   "metadata": {},
   "source": [
    "\n",
    "The output of this post-conversion step should help determine whether to avoid using the content for chunking entirely or to manually edit it before proceeding with chunking.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafad55e-a4c0-4d6e-9da0-49519fa9bf74",
   "metadata": {},
   "source": [
    "## Chunking\n",
    "\n",
    "The goal of chunking the converted documents is to provide the teacher model small and logical pieces of the source document to generate data off of.\n",
    "\n",
    "In this notebook we are doing chunking with [Docling](https://docling-project.github.io/docling/examples/hybrid_chunking/#hybrid-chunking).\n",
    "\n",
    "The input to this notebook is a docling JSON file created after a docling conversion, or a directory of docling JSON files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2482060c-a49f-4345-aa47-d54301939387",
   "metadata": {},
   "source": [
    "### Initialize the Chunker\n",
    "\n",
    "Docling provides two chunkers, the `HierarchicalChunker` and the `HybridChunker`.\n",
    "The `HierarchicalChunker` creates chunks based on the hierarchy in the Docling document\n",
    "\n",
    "The `HybridChunker` builds on the `HierarchicalChunker` and by making it tokenization aware.\n",
    "\n",
    "The `HybridChunker` has options for a `tokenizer`, the `max_tokens` in a chunk, and whether to merge undersized peer chunks. Uncomment the commented out code to configure these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50df9d91-add4-46a1-a69d-0f7f9f69542e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from docling_core.transforms.chunker.tokenizer.huggingface import HuggingFaceTokenizer\n",
    "#from transformers import AutoTokenizer\n",
    "\n",
    "from docling.chunking import HybridChunker\n",
    "\n",
    "#EMBED_MODEL_ID = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "#MAX_TOKENS = 1024\n",
    "#\n",
    "# tokenizer = HuggingFaceTokenizer(\n",
    "#     tokenizer=AutoTokenizer.from_pretrained(EMBED_MODEL_ID),\n",
    "#     max_tokens=MAX_TOKENS,  # optional, by default derived from `tokenizer` for HF case\n",
    "#     merge_peers=True # \n",
    "# )\n",
    "\n",
    "chunker = HybridChunker(\n",
    "    #tokenizer=tokenizer,\n",
    "    #merge_peers=True,  # whether to merge undersized chunks - defaults to True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ce1d6f-b8d3-470c-b3c9-675911f0ee92",
   "metadata": {},
   "source": [
    "### Load and chunk the converted docling document\n",
    "\n",
    "Next lets convert the document we want to chunk up into a Docling Document.\n",
    "\n",
    "All chunks are saved to a JSON file called chunks.jsonl in the `chunks` directory in your contribution. This file is one of the inputs father below when we create the seed dataset for SDG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db983c05-4aa6-4261-9283-2adab69bfbd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 2025 chunks from 2022-nfl-rulebook-final\n",
      "{'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/2', 'parent': {'cref': '#/body'}, 'children': [], 'content_layer': <ContentLayer.BODY: 'body'>, 'label': <DocItemLabel.TEXT: 'text'>, 'prov': [{'page_no': 1, 'bbox': {'l': 212.453, 't': 221.80700000000002, 'r': 399.901, 'b': 209.88400000000001, 'coord_origin': <CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>}, 'charspan': (0, 27)}]}], 'headings': ['2022 OFFICIAL PLAYING RULES OF THE NATIONAL FOOTBALL LEAGUE'], 'captions': None, 'origin': {'mimetype': 'application/pdf', 'binary_hash': 3168899155016516390, 'filename': '2022-nfl-rulebook-final.pdf', 'uri': None}}\n",
      "{'schema_name': 'docling_core.transforms.chunker.DocMeta', 'version': '1.0.0', 'doc_items': [{'self_ref': '#/texts/2', 'parent': {'$ref': '#/body'}, 'children': [], 'content_layer': 'body', 'label': 'text', 'prov': [{'page_no': 1, 'bbox': {'l': 212.453, 't': 221.80700000000002, 'r': 399.901, 'b': 209.88400000000001, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0, 27]}]}], 'headings': ['2022 OFFICIAL PLAYING RULES OF THE NATIONAL FOOTBALL LEAGUE'], 'origin': {'mimetype': 'application/pdf', 'binary_hash': 3168899155016516390, 'filename': '2022-nfl-rulebook-final.pdf'}}\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(chunk.meta.model_dump())\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(chunk.meta.export_json_dict())\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m\n\u001b[32m     24\u001b[39m c = \u001b[38;5;28mdict\u001b[39m(chunk=chunker.contextualize(chunk=chunk), file=doc.document.name,metadata=chunk.meta.export_json_dict())\n\u001b[32m     25\u001b[39m all_chunks.append(c)\n",
      "\u001b[31mException\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "for contribution_dir in contribution_dirs:\n",
    "    conversion_dir = contribution_dir / \"conversion\"\n",
    "    json_files = list(conversion_dir.glob(\"*.json\"))\n",
    "    chunking_output_dir = contribution_dir / \"chunks\"\n",
    "    chunking_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    all_chunks = []\n",
    "    \n",
    "    for file in json_files:\n",
    "        # reconvert the docling JSON for chunking\n",
    "        doc = DocumentConverter().convert(source=file)\n",
    "        \n",
    "        chunk_iter = chunker.chunk(dl_doc=doc.document)\n",
    "        chunk_objs = list(chunk_iter)\n",
    "    \n",
    "        print(f\"Extracted {len(chunk_objs)} chunks from {doc.document.name}\")\n",
    "        \n",
    "        for chunk in chunk_objs:\n",
    "            print(chunk.meta.model_dump())\n",
    "            print(chunk.meta.export_json_dict())\n",
    "            c = dict(chunk=chunker.contextualize(chunk=chunk), file=doc.document.name,metadata=chunk.meta.export_json_dict())\n",
    "            all_chunks.append(c)\n",
    "\n",
    "        chunks_file_path = chunking_output_dir / \"chunks.jsonl\"\n",
    "        with open(chunks_file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            for chunk in all_chunks:\n",
    "                json.dump(chunk, file)\n",
    "                file.write(\"\\n\")\n",
    "            print(f\"Path of chunks JSON is: {Path(chunks_file_path).resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb38545-eb84-4923-8fc4-d10ed08eab26",
   "metadata": {},
   "source": [
    "### View the Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fdf34c7-9829-43d2-bf9f-7d1d55bb6a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_gen = iter(all_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811992ac",
   "metadata": {},
   "source": [
    "To view the chunks one by one, rerun the following cell. As you can see the document is broken into small pieces with metadata about the chunk based on the document's format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee9a8531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Field Markings\n",
      "9. A white arrow is to be placed on the ground adjacent to the top portion of each number (with the exception of the 50) with the point formed by the two longer sides pointing toward the goal line. The two longer sides measure 36 inches each, while the  side measures 18 inches. The 18-inch crossfield side is to start 15 inches below the top, and 6 inches from the goalward edge of each outer number (except the 50).\n",
      "10. The location  of  the  inbounds  lines  is  70'9'  for  professional  football, 60'0' for  college  football.  On  fields  used  primarily  by  the  NFL,  the professional inbounds lines should be 4 inches wide by 2 feet long. Alternate college lines, if they are to be included, should be 4 inches wide by 1 foot long.\n",
      "11. Care  must  be  exercised  in  any  end  zone  marking,  decoration,  or  club identification at the 50-yard line, that said marks or decorations do not in any way cause confusion as to delineation of goal lines, sidelines, and end lines. Such markings or decorations must be approved by the Commissioner.\n"
     ]
    }
   ],
   "source": [
    "print(next(chunk_gen)['chunk'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a510f8c7-8cd3-4867-8742-9f4f9cda9e9f",
   "metadata": {},
   "source": [
    "## Authoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86c48e52-cda7-48ac-84dc-0b844aed5f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq docling-sdg\n",
    "\n",
    "# TODO: replace with above after https://github.com/docling-project/docling-sdg/pull/31 merges\n",
    "#!pip install -qq git+https://github.com/anastasds/docling-sdg@d15de2c5a81bfe166f66f412fc4b23728065f396"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2a165c38-843b-4c89-a8ad-6195b998e284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking and filtering document 2022-nfl-rulebook-final\n",
      "Created dataset 2022-nfl-rulebook-final with 966 QA chunks\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import yaml\n",
    "from docling_core.transforms.chunker.hierarchical_chunker import DocChunk, DocMeta\n",
    "from docling_sdg.qa.utils import get_qa_chunks\n",
    "\n",
    "filters = [\n",
    "    lambda chunk: len(str(chunk.text)) > 500\n",
    "]\n",
    "\n",
    "dataset = {}\n",
    "for contribution_name, contribution_dir in zip(contribution_names, contribution_dirs):\n",
    "    dataset[contribution_name] = {}\n",
    "\n",
    "    chunks_jsonl_path = contribution_dir / \"chunks\" / \"chunks.jsonl\"\n",
    "    \n",
    "    # create a list docs with each document and list[BaseChunk] for it out of chunks.jsonl\n",
    "    docs = []\n",
    "    with open(chunks_jsonl_path, 'r') as file:  # khaled was here\n",
    "        for line in file:\n",
    "            file_in_docs = False\n",
    "            entry = yaml.safe_load(line)\n",
    "            meta = DocMeta(**entry['metadata'])\n",
    "            chunk = DocChunk(text=entry['chunk'], meta=meta)\n",
    "            for doc in docs:\n",
    "                if doc[\"file\"] == entry['file']:\n",
    "                    doc[\"chunk_objs\"].append(chunk)\n",
    "                    file_in_docs = True\n",
    "                    break\n",
    "                    \n",
    "            if file_in_docs == False:\n",
    "                doc = dict(file=entry['file'], chunk_objs=[chunk])\n",
    "                docs.append(doc)\n",
    "            \n",
    "\n",
    "    for doc in docs:\n",
    "        print(f\"Chunking and filtering document {doc['file']}\")\n",
    "        \n",
    "        # get_qa_chunks expects a list[DocChunk] which we already have from doc[\"chunk_objs\"] in the chunking section\n",
    "        qa_chunks = []\n",
    "        for qa_chunk in get_qa_chunks(doc[\"file\"], doc[\"chunk_objs\"], filters):\n",
    "            qa_chunks.append(qa_chunk.export_json_dict())\n",
    "        dataset[contribution_name][doc[\"file\"]] = qa_chunks\n",
    "\n",
    "        with open(contribution_dir / \"authoring\" / f\"qagen-{doc['file']}.json\", \"w\") as f:\n",
    "            json.dump(dataset[contribution_name], f)\n",
    "        \n",
    "        print(f\"Created dataset {doc['file']} with {len(qa_chunks)} QA chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65ec755-e3de-40ab-bf3a-23ebb29a705d",
   "metadata": {},
   "source": [
    "### Initialize QA generator, supplying details for which model to use\n",
    "\n",
    "GenerateOptions controls which model is used for QA generation by setting generate_options.provider below. Three options are available:\n",
    "\n",
    "* LlmProviders.WATSONX for watsonx\n",
    "* LlmProviders.OPENAI for OpenAI\n",
    "* LlmProviders.OPENAI_LIKE for any model provider with OpenAI compatible APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874d4de8",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "API_KEY = \"none\"  # the API access key for your account ( cannot be empty )\n",
    "API_URL = \"http://127.0.0.1:11434/v1\"  # the URL of your model's API\n",
    "MODEL_ID = \"granite3.3\" # the name of your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b702267e-f550-4bc2-bce4-c0fcecbbd292",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling_sdg.qa.generate import Generator\n",
    "from docling_sdg.qa.base import GenerateOptions, LlmProvider\n",
    "from pydantic import SecretStr\n",
    "\n",
    "generate_options = GenerateOptions(project_id=\"project_id\")\n",
    "generate_options.provider = LlmProvider.OPENAI_LIKE\n",
    "generate_options.api_key = SecretStr(API_KEY)\n",
    "generate_options.url = API_URL\n",
    "generate_options.model_id = MODEL_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919199c0-3747-409a-85ab-0155ef3ebe9d",
   "metadata": {},
   "source": [
    "### Configure subset selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f1197d4e-8354-45e3-9ec9-85c78ba36548",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CHUNKS_PER_FILE_TO_SELECT_FOR_AUTHORING = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2421d07-3e6c-4355-95f4-da8e157557c7",
   "metadata": {},
   "source": [
    "### Run QA generation on selected chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e57edff5-9a13-47fb-9248-9140ae5baaca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing chunks that looks like:\n",
      "PREFACE\n",
      "This edition of the Official Playing Rules of the  National Football League  contains all current rules governing the playing of professional football that are in effect for the 2022 NFL season. Member clubs of the League may amend the rules from time to time, pursuant to the applicable voting procedures of the NFL Constitution and Bylaws.\n",
      "Any intra-League dispute or call for interpretation in connection with these rules will be decided by the Commissioner of the League, whose ruling will be final.\n",
      "Because inter-conference games are played throughout the preseason, regular season, and postseason in  the  NFL, all  rules contained in  this  book apply uniformly to both the American and National Football Conferences.\n",
      "Where the word 'illegal' appears in this rule book, it is an institutional term of art pertaining strictly to actions that violate NFL playing rules. It is not meant to connote illegality under any public law or the rules or regulations of any other organization.\n",
      "Selected 2 contexts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "No Mapping Rule matched",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotFoundError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSelected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(selected_chunks)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m contexts\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m Path.unlink(generate_options.generated_file, missing_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m results = \u001b[43mgen\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_from_chunks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mselected_chunks\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# automatically saves to file\u001b[39;00m\n\u001b[32m     14\u001b[39m generated_files.append(generate_options.generated_file)\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdoc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults.status\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/ilab/examples/venv/lib/python3.11/site-packages/docling_sdg/qa/generate.py:128\u001b[39m, in \u001b[36mGenerator.generate_from_chunks\u001b[39m\u001b[34m(self, stored_chunks)\u001b[39m\n\u001b[32m    125\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    127\u001b[39m \u001b[38;5;66;03m# Generate question\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m question, question_prompt = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_from_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontext_str\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquestion_types\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mqac_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPromptTypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mQUESTION\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m question \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m question_prompt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/ilab/examples/venv/lib/python3.11/site-packages/docling_sdg/qa/generate.py:83\u001b[39m, in \u001b[36mGenerator.generate_from_prompt\u001b[39m\u001b[34m(self, key_dict, question_types, prompt_type)\u001b[39m\n\u001b[32m     79\u001b[39m prompt_template = PromptTemplate(template=template)\n\u001b[32m     80\u001b[39m prompt = format_string(prompt_template.template, **key_dict).strip()\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     84\u001b[39m     .replace(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     85\u001b[39m     .strip(),\n\u001b[32m     86\u001b[39m     prompt.strip(),\n\u001b[32m     87\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/ilab/examples/venv/lib/python3.11/site-packages/docling_sdg/qa/utils.py:203\u001b[39m, in \u001b[36mChatAgent.ask\u001b[39m\u001b[34m(self, question, max_tokens)\u001b[39m\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mask\u001b[39m(\u001b[38;5;28mself\u001b[39m, question: \u001b[38;5;28mstr\u001b[39m, max_tokens: \u001b[38;5;28mint\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m     answer = \u001b[38;5;28mstr\u001b[39m(response)\n\u001b[32m    205\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m answer\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/ilab/examples/venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py:324\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    321\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m324\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    325\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    326\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    327\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/ilab/examples/venv/lib/python3.11/site-packages/llama_index/llms/openai_like/base.py:160\u001b[39m, in \u001b[36mOpenAILike.chat\u001b[39m\u001b[34m(self, messages, **kwargs)\u001b[39m\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.metadata.is_chat_model:\n\u001b[32m    159\u001b[39m     prompt = \u001b[38;5;28mself\u001b[39m.messages_to_prompt(messages)\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     completion_response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcomplete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatted\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m completion_response_to_chat_response(completion_response)\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().chat(messages, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/ilab/examples/venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py:324\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    321\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m324\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    325\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    326\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    327\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/ilab/examples/venv/lib/python3.11/site-packages/llama_index/llms/openai_like/base.py:145\u001b[39m, in \u001b[36mOpenAILike.complete\u001b[39m\u001b[34m(self, prompt, formatted, **kwargs)\u001b[39m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m formatted:\n\u001b[32m    143\u001b[39m     prompt = \u001b[38;5;28mself\u001b[39m.completion_to_prompt(prompt)\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcomplete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/ilab/examples/venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py:324\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    321\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m324\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    325\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    326\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    327\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/ilab/examples/venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py:431\u001b[39m, in \u001b[36mllm_completion_callback.<locals>.wrap.<locals>.wrapped_llm_predict\u001b[39m\u001b[34m(_self, *args, **kwargs)\u001b[39m\n\u001b[32m    422\u001b[39m event_id = callback_manager.on_event_start(\n\u001b[32m    423\u001b[39m     CBEventType.LLM,\n\u001b[32m    424\u001b[39m     payload={\n\u001b[32m   (...)\u001b[39m\u001b[32m    428\u001b[39m     },\n\u001b[32m    429\u001b[39m )\n\u001b[32m    430\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m431\u001b[39m     f_return_val = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_self\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    432\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    433\u001b[39m     callback_manager.on_event_end(\n\u001b[32m    434\u001b[39m         CBEventType.LLM,\n\u001b[32m    435\u001b[39m         payload={EventPayload.EXCEPTION: e},\n\u001b[32m    436\u001b[39m         event_id=event_id,\n\u001b[32m    437\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/ilab/examples/venv/lib/python3.11/site-packages/llama_index/llms/openai/base.py:405\u001b[39m, in \u001b[36mOpenAI.complete\u001b[39m\u001b[34m(self, prompt, formatted, **kwargs)\u001b[39m\n\u001b[32m    403\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    404\u001b[39m     complete_fn = \u001b[38;5;28mself\u001b[39m._complete\n\u001b[32m--> \u001b[39m\u001b[32m405\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcomplete_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/ilab/examples/venv/lib/python3.11/site-packages/llama_index/llms/openai/base.py:111\u001b[39m, in \u001b[36mllm_retry_decorator.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    102\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m    104\u001b[39m retry = create_retry_decorator(\n\u001b[32m    105\u001b[39m     max_retries=max_retries,\n\u001b[32m    106\u001b[39m     random_exponential=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    109\u001b[39m     max_seconds=\u001b[32m20\u001b[39m,\n\u001b[32m    110\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/ilab/examples/venv/lib/python3.11/site-packages/tenacity/__init__.py:338\u001b[39m, in \u001b[36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[39m\u001b[34m(*args, **kw)\u001b[39m\n\u001b[32m    336\u001b[39m copy = \u001b[38;5;28mself\u001b[39m.copy()\n\u001b[32m    337\u001b[39m wrapped_f.statistics = copy.statistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/ilab/examples/venv/lib/python3.11/site-packages/tenacity/__init__.py:477\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    475\u001b[39m retry_state = RetryCallState(retry_object=\u001b[38;5;28mself\u001b[39m, fn=fn, args=args, kwargs=kwargs)\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m     do = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/ilab/examples/venv/lib/python3.11/site-packages/tenacity/__init__.py:378\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    376\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m     result = \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/ilab/examples/venv/lib/python3.11/site-packages/tenacity/__init__.py:400\u001b[39m, in \u001b[36mBaseRetrying._post_retry_check_actions.<locals>.<lambda>\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    398\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_post_retry_check_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, retry_state: \u001b[33m\"\u001b[39m\u001b[33mRetryCallState\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    399\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.iter_state.is_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.retry_run_result):\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m         \u001b[38;5;28mself\u001b[39m._add_action_func(\u001b[38;5;28;01mlambda\u001b[39;00m rs: \u001b[43mrs\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutcome\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    401\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    403\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.after \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/ilab/examples/venv/lib/python3.11/site-packages/tenacity/__init__.py:480\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    478\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m         result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[32m    482\u001b[39m         retry_state.set_exception(sys.exc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/ilab/examples/venv/lib/python3.11/site-packages/llama_index/llms/openai/base.py:574\u001b[39m, in \u001b[36mOpenAI._complete\u001b[39m\u001b[34m(self, prompt, **kwargs)\u001b[39m\n\u001b[32m    571\u001b[39m \u001b[38;5;28mself\u001b[39m._update_max_tokens(all_kwargs, prompt)\n\u001b[32m    573\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.reuse_client:\n\u001b[32m--> \u001b[39m\u001b[32m574\u001b[39m     response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    575\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    576\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    577\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mall_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    578\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    579\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    580\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m client:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/ilab/examples/venv/lib/python3.11/site-packages/openai/_utils/_utils.py:287\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    285\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    286\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/ilab/examples/venv/lib/python3.11/site-packages/openai/resources/completions.py:541\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, model, prompt, best_of, echo, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, seed, stop, stream, stream_options, suffix, temperature, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    512\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    513\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    514\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    539\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m    540\u001b[39m ) -> Completion | Stream[Completion]:\n\u001b[32m--> \u001b[39m\u001b[32m541\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    542\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    543\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    546\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    547\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbest_of\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    548\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mecho\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mecho\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    549\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    552\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    553\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    554\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    555\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    556\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    558\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    559\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msuffix\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    560\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    561\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    564\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m    565\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m    566\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    568\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    569\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    570\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    571\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mCompletion\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/ilab/examples/venv/lib/python3.11/site-packages/openai/_base_client.py:1239\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1225\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1226\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1227\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1234\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1235\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1236\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1237\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1238\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1239\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/ilab/examples/venv/lib/python3.11/site-packages/openai/_base_client.py:1034\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1031\u001b[39m             err.response.read()\n\u001b[32m   1033\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1034\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1036\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1038\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mNotFoundError\u001b[39m: No Mapping Rule matched"
     ]
    }
   ],
   "source": [
    "import random #TODO: replace random sampling with subset selection\n",
    "for name in contribution_names:\n",
    "    for contribution_dir in contribution_dirs:\n",
    "        generated_files = []\n",
    "        for doc, chunks in dataset[name].items():\n",
    "            generate_options.generated_file = contribution_dir / \"authoring\" / f\"qagen-{doc}.json\"\n",
    "            gen = Generator(generate_options=generate_options)\n",
    "            print(f\"processing chunks that looks like:\\n{chunks[0].text}\")\n",
    "            selected_chunks = random.sample(chunks, NUM_CHUNKS_PER_FILE_TO_SELECT_FOR_AUTHORING)\n",
    "            print(f\"Selected {len(selected_chunks)} contexts\")\n",
    "        \n",
    "            Path.unlink(generate_options.generated_file, missing_ok=True)\n",
    "            results = gen.generate_from_chunks(selected_chunks) # automatically saves to file\n",
    "            generated_files.append(generate_options.generated_file)\n",
    "        \n",
    "            print(f\"{doc}: {results.status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea64b8f0-dd6c-4776-8646-9731433f909b",
   "metadata": {},
   "source": [
    "### Read generated QAs and restructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df2c533-30d7-4c30-9907-7c5655fd2246",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import yaml\n",
    "from textwrap import wrap\n",
    "\n",
    "qnas = {}\n",
    "chunk_id_to_text = {}\n",
    "for file in generated_files:\n",
    "    with open(file, \"rt\") as f:\n",
    "        for line in f.readlines():\n",
    "            entry = json.loads(line)\n",
    "            chunk_id = entry['chunk_id']\n",
    "            if chunk_id not in chunk_id_to_text:\n",
    "                chunk_id_to_text[chunk_id] = entry['context']\n",
    "            if chunk_id not in qnas:\n",
    "                qnas[chunk_id] = []\n",
    "            qnas[chunk_id].append({'question': entry['question'], 'answer': entry['answer']})\n",
    "\n",
    "print(f\"Generated QA pairs for {len(qnas)} contexts\")\n",
    "print(list(qnas.values())[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafa8927-e56c-448b-b88b-f8d854c25d4d",
   "metadata": {},
   "source": [
    "### Output qna.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f26460-737f-4940-b58a-ef6caea313d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following creates a data structure for outputting in the expected format for qna.yaml\n",
    "# TODO: extract into utils library\n",
    "\n",
    "def str_presenter(dumper, data):\n",
    "  if len(data.splitlines()) > 1:  # check for multiline string\n",
    "    return dumper.represent_scalar('tag:yaml.org,2002:str', data, style='|')\n",
    "  elif len(data) > 80:\n",
    "    data = \"\\n\".join(wrap(data, 80))\n",
    "    return dumper.represent_scalar('tag:yaml.org,2002:str', data, style='|')\n",
    "  return dumper.represent_scalar('tag:yaml.org,2002:str', data)\n",
    "\n",
    "yaml.add_representer(str, str_presenter)\n",
    "\n",
    "# to use with safe_dump:\n",
    "yaml.representer.SafeRepresenter.add_representer(str, str_presenter)\n",
    "\n",
    "class IndentedDumper(yaml.Dumper):\n",
    "    def increase_indent(self, flow=False, indentless=False):\n",
    "        return super(IndentedDumper, self).increase_indent(flow, False)\n",
    "\n",
    "for contribution in contributions:\n",
    "    qna_output_path = AUTHORING_OUTPUT_DIR / contribution[\"prefix\"] / \"qna.yaml\"\n",
    "    \n",
    "    data = {'seed_examples': []}\n",
    "    for chunk_id, context in contribution[\"chunk_id_to_text\"].items():\n",
    "        data['seed_examples'].append({\n",
    "            'context': context,\n",
    "            'questions_and_answers': [\n",
    "                {\n",
    "                    'question': example['question'],\n",
    "                    'answer': example['answer'],\n",
    "                } for example in contribution[\"qnas\"][chunk_id]\n",
    "            ]\n",
    "        })\n",
    "    \n",
    "    data['document_outline'] = contribution[\"summary\"]\n",
    "    data['domain'] = contribution[\"domain\"]\n",
    "    \n",
    "    Path.unlink(qna_output_path, missing_ok=True) # shouldn't be necessary but was. jupyter caching thing?\n",
    "    with open(qna_output_path, 'w') as yaml_file:\n",
    "        yaml.dump(data, yaml_file, Dumper=IndentedDumper, default_flow_style=False, sort_keys=False, width=80)\n",
    "    \n",
    "    print(f\"qna.yaml saved to: {qna_output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9ea149-844b-4330-90ec-d0ca7ab12b90",
   "metadata": {},
   "source": [
    "### View generated qna.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1293d445-b826-4b92-ad20-9b121ac60e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "for contribution in contributions:\n",
    "    qna_output_path = AUTHORING_OUTPUT_DIR / contribution[\"prefix\"] / \"qna.yaml\"\n",
    "\n",
    "    with open(qna_output_path) as yaml_file:\n",
    "        print(f\"========= qna.yaml at {qna_output_path} ==========\")\n",
    "        print(yaml_file.read())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c574f96-5860-48b9-b4ac-01d367c7717b",
   "metadata": {},
   "source": [
    "### Revise QAs\n",
    "\n",
    "Open the generated `qna.yaml` in your preferred text editor to ensure the quality of generated questions and answers. If the generation step has failed to generated three questions and answers for each of five contexts, supplant until that required number of QA pairs is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f101076-a50f-49ea-a83b-46eaa8b39cc4",
   "metadata": {},
   "source": [
    "## Create Seed Dataset for SDG\n",
    "\n",
    "This section combines the contents from the qna.yaml and the chunks from the source document to create a seed dataset for the synthetic data generation process.\n",
    "\n",
    "To run this step you need a directory that contains `chunks.jsonl` and a `qna.yaml` in the same directory.\n",
    "\n",
    "This step outputs a seed.jsonl file in the SDG_OUTPUT_DIR that you set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c6e31b-e8a9-406c-b2dc-27433c8fd8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq datasets transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2c9ed2-8ba8-4959-8e01-81625b81d286",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.create_seed_dataset import get_seed_dataset, safe_concatenate_datasets\n",
    "\n",
    "contribution_datasets = []\n",
    "for contribution in contributions:\n",
    "    chunks_dir = Path(CHUNKING_OUTPUT_DIR / contribution[\"prefix\"])\n",
    "    qna_dir = Path(AUTHORING_OUTPUT_DIR / contribution[\"prefix\"])\n",
    "    seed_data = get_seed_dataset(chunks_dir, qna_dir)\n",
    "    contribution_datasets.append(seed_data)\n",
    "    # TODO write seed.jsonl under contributions dir\n",
    "\n",
    "final_seed_data = safe_concatenate_datasets(contribution_datasets)\n",
    "output_path = f'{SDG_OUTPUT_DIR}/seed_data.jsonl'\n",
    "final_seed_data.to_json(output_path, orient='records', lines=True)\n",
    "\n",
    "print(f\"Seed data contains {final_seed_data.data.num_rows} rows\")\n",
    "print(f\"Results saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ff36f4-19fc-4a27-b51a-3688e7b630e4",
   "metadata": {},
   "source": [
    "### Inspect the seed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6936825-31c1-4b46-a1af-2fb46f50158d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(seed_data.data.table.slice(length=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a8fcdb-8035-4f30-b856-46afe9f928a1",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "To recap, given a source document in PDF format, this notebook:\n",
    "\n",
    "1. Converted the document using document and saved it to JSON for inspection\n",
    "2. Split the extracted text into chunks\n",
    "3. Generated QA pairs for a subset of those chunks\n",
    "4. Created a `qna.yaml` available for inspection and revision\n",
    "5. Combined the chunks and `qna.yaml` to create a `seed_data.jsonl` for use with SDG\n",
    "\n",
    "The next step is to use the resulting `seed_data.jsonl` for SDG, such as illustrated in [this notebook](https://github.com/Red-Hat-AI-Innovation-Team/sdg_hub/blob/main/examples/instructlab/knowledge/knowledge_generation_and_mixing.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
