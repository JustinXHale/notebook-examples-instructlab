{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d378e39b-4de3-4578-867b-e142c5ce23d6",
   "metadata": {},
   "source": [
    "# Subset Selection on Text Chunks\n",
    "\n",
    "This notebook demonstrates how to perform subset selection on a set of text chunks specified in a `chunks.jsonl` file, with an example included in the `data/` subdirectory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7b5b3c-cc68-4418-8b79-6caeddfeb1d1",
   "metadata": {},
   "source": [
    "## Load the dataset\n",
    "\n",
    "First, we load the `chunks.jsonl` file. It is expected that the text chunks are in a key called `chunk`, with all other JSON key/values being metadata to have been preserved throughout this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39792a86-af14-4ce8-bb25-287c0f16d766",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "chunk_lookup = {}\n",
    "with open('data/chunks.jsonl', encoding='utf-8') as fin:\n",
    "    os.makedirs('tmp', exist_ok=True)\n",
    "    for line in fin.readlines():\n",
    "        chunk_json = json.loads(line)\n",
    "        chunk_lookup[chunk_json['chunk']] = chunk_json\n",
    "\n",
    "print(f'Read {len(chunk_lookup)} chunks')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058dcbc4-097d-412f-a8b6-18ce4ee844fc",
   "metadata": {},
   "source": [
    "# Preparation for performing subset selection\n",
    "\n",
    "We check out the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0594d7bb-228b-468a-99a0-38c14651a562",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone git@github.com:krishnatejakk/DataCurate4LLMs.git\n",
    "%cd DataCurate4LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2c8b60-daa9-48e4-b680-0b59588609ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "seed = random.randint(0,10000)\n",
    "\n",
    "config = f\"\"\"{{\n",
    "    \"output_dir\": \"output\",\n",
    "    \"encoder_model\": \"BAAI/bge-large-en-v1.5\",\n",
    "    \"encoder_type\": \"bge\",\n",
    "    \"instruction\": \"Generate embeddings that capture the semantic meaning of text segments across multiple domains, ensuring g\\\n",
    "eneralization and suitability for clustering based on semantic similarity.\",\n",
    "    \"query_description\": \"default\",\n",
    "    \"templates\": {{\n",
    "        \"default\": \"{{{{ chunk }}}}\"\n",
    "    }},\n",
    "    \"template_name\": \"default\",\n",
    "    \"num_folds\": 1,\n",
    "    \"num_gpus\": 1,\n",
    "    \"subset_sizes\": [\"5\"],\n",
    "    \"epsilon\": 0.01,\n",
    "    \"seed\": {seed}\n",
    "}}\"\"\"\n",
    "\n",
    "config_path = 'subset_config.json'\n",
    "with open(config_path, 'w') as f:\n",
    "    f.write(config)\n",
    "\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7efc49a-c08d-42f9-ad7a-b0723b49982f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed -i -e 's ^faiss-gpu$ faiss-gpu-cu12 g' requirements.txt # fix faiss dependency; yes you can use spaces as delimiters for sed expressions\n",
    "!pip install -qq -r requirements.txt\n",
    "!pip install -qq submodlib-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b086790-5a9f-4e2f-b130-2d9f9f8a0319",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python data_subset_selection.py --input_files '../data/chunks.jsonl' --output_dir '../data' --config '../subset_config.json' --retry_delay 1 --subset_sizes 5 --num_gpus 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b5622c-2101-47df-b366-2afb137cdd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('../data/chunks/chunks_samples_5_subset.jsonl') as fin:\n",
    "    with open('../data/selected_chunks.jsonl','w') as fout:\n",
    "        for line in fin.readlines():\n",
    "            selected_chunk = json.loads(line)['chunk']\n",
    "            original_chunk = chunk_lookup[selected_chunk]\n",
    "            fout.write(json.dumps(original_chunk) + \"\\n\")\n",
    "\n",
    "with open('../data/selected_chunks.jsonl') as final:\n",
    "    for line in final.readlines():\n",
    "        print(json.dumps(json.loads(line), indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
