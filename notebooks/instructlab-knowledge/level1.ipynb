{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5da70a08-1895-4d1f-8f50-93e2134b2e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# design goals:\n",
    "#\n",
    "# - understandability\n",
    "# - modularity\n",
    "# - configurability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780bb24e-c093-42ae-9b28-97e1af47ef25",
   "metadata": {},
   "source": [
    "# config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23353c1-78e9-48e3-86c6-179618874c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe7d6f2-e089-4a19-85a8-5c30d598f563",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKSPACE_DIR=\"workspaces/default\"\n",
    "\n",
    "CONVERSION_OUTPUT_DIR = Path(f\"{WORKSPACE_DIR}/conversion\")\n",
    "CONVERSION_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CHUNKING_OUTPUT_DIR = Path(f\"{WORKSPACE_DIR}/chunking\")\n",
    "CHUNKING_OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "SEED_EXAMPLE_OUTPUT_DIR = Path(f\"{WORKSPACE_DIR}/seed-examples\")\n",
    "SEED_EXAMPLE_OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "SDG_OUTPUT_DIR = Path(f\"{WORKSPACE_DIR}/sdg\")\n",
    "SDG_OUTPUT_DIR.mkdir(exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21ef7b6-4cea-412d-9535-e378c8a70193",
   "metadata": {},
   "source": [
    "# conversion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66042fef-89dc-47e3-b2ce-bee1adec791a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install docling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d150ec43-5b76-4fb8-898b-ccb9b6a7a906",
   "metadata": {},
   "source": [
    "This notebook uses [Docling](https://docling-project.github.io/docling/examples/custom_convert/) to convert any type of document into a Docling Document. A Docling Document is the representation of the document after conversion that can be exported as JSON. The JSON output of this notebook can then be used in others such as one that uses Docling's chunking methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ffffd3-7870-47e3-846c-63873dbc2d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.document_converter import DocumentConverter, ConversionError, PdfFormatOption\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
    "import json\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba023916-c3ba-4397-a9bd-3ab59eae372d",
   "metadata": {},
   "source": [
    "First we set the paths for the documents we want to convert and where the JSON output should live."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549fa984-f023-4816-9683-b5956c7582c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_path = Path(\"/path/to/pdf\")\n",
    "output_dir = Path(\"/output/path\")\n",
    "\n",
    "files = []\n",
    "\n",
    "if doc_path.is_file():\n",
    "    files = [doc_path]\n",
    "else:\n",
    "    files = list(doc_path.rglob(\"*.pdf\"))\n",
    "print(f\"Files to convert: {files}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadc9718-e7b7-4300-9600-d5e2407f0dfb",
   "metadata": {},
   "source": [
    "Next we set the configuration options for our conversion pipeline. The PDF Conversion options set here are the defaults. More information about pipeline configuration can be found on Docling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480de47e-2916-434a-9e62-26681eb6f246",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_options = PdfPipelineOptions()\n",
    "\n",
    "doc_converter = DocumentConverter(\n",
    "     format_options={\n",
    "         InputFormat.PDF: PdfFormatOption(\n",
    "             pipeline_options=pipeline_options\n",
    "         )\n",
    "     }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf876f0-dba6-475a-b439-21aa58cd72b9",
   "metadata": {},
   "source": [
    "Finally we convert every document into Docling JSON as long as it is a valid file type to be converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675d116e-43c2-453d-a004-f182afb05589",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    try:\n",
    "        doc = doc_converter.convert(source=file).document\n",
    "        doc_dict = doc.export_to_dict()\n",
    "        json_output_path = output_dir / f\"{file.stem}.json\"\n",
    "        with open(json_output_path, \"w\") as f:\n",
    "            json.dump(doc_dict, f)\n",
    "            print(f\"Path of JSON output is: {Path(json_output_path).resolve()}\")\n",
    "    except ConversionError as e:\n",
    "        print(f\"Skipping file {file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9bbcbd-9bdd-474e-9d44-5c9d5a2c03e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a165c38-843b-4c89-a8ad-6195b998e284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# authoring\n",
    "\n",
    "# TODO: plug into docs\n",
    "\n",
    "filters = [\n",
    "    lambda chunk: len(str(chunk.text)) > 500\n",
    "]\n",
    "\n",
    "dataset = {}\n",
    "for doc in docs:\n",
    "    print(f\"Chunking and filtering document {doc.document.name}\")\n",
    "\n",
    "    chunks = list(chunker.chunk(dl_doc=doc.document))\n",
    "    qa_chunks = list(get_qa_chunks(doc.document.name, chunks, filters))\n",
    "    dataset[doc.document.name] = qa_chunks\n",
    "    \n",
    "    print(f\"Created dataset {doc.document.name} with {len(qa_chunks)} QA chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65ec755-e3de-40ab-bf3a-23ebb29a705d",
   "metadata": {},
   "source": [
    "#Initialize QA generator, supplying details for which model to use\n",
    "\n",
    "GenerateOptions controls which model is used for QA generation by setting generate_options.provider below. Three options are available:\n",
    "\n",
    "* LlmProviders.WATSONX for watsonx\n",
    "* LlmProviders.OPENAI for OpenAI\n",
    "* LlmProviders.OPENAI_LIKE for any model provider with OpenAI compatible APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216b50a7-889f-4432-a29b-5a8fd55177d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install docling-sdg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b702267e-f550-4bc2-bce4-c0fcecbbd292",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling_sdg.qa.generate import Generator\n",
    "from docling_sdg.qa.base import GenerateOptions, LlmProviders\n",
    "\n",
    "generate_options = GenerateOptions(api_key=\"fake\", project_id=\"project_id\")\n",
    "generate_options.provider = LlmProviders.OPENAI_LIKE\n",
    "generate_options.api_key = \"fake\"\n",
    "generate_options.model_id = \"mixtral\" # for local ollama\n",
    "generate_options.generated_file = f\"data/chunks-{filename_base}.jsonl\"\n",
    "\n",
    "gen = Generator(generate_options=generate_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57edff5-9a13-47fb-9248-9140ae5baaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc, chunks in dataset.items():\n",
    "    print(f\"processing chunks that looks like:\\n{chunks[0].text}\")\n",
    "    results = gen.generate_from_chunks(chunks)\n",
    "    print(f\"{doc}: {results.status}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea64b8f0-dd6c-4776-8646-9731433f909b",
   "metadata": {},
   "source": [
    "# Read generated QAs and restructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df2c533-30d7-4c30-9907-7c5655fd2246",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import yaml\n",
    "from textwrap import wrap\n",
    "\n",
    "qnas = {}\n",
    "chunk_id_to_text = {}\n",
    "with open(generate_options.generated_file, \"rt\") as f:\n",
    "    for line in f.readlines():\n",
    "        entry = json.loads(line)\n",
    "        chunk_id = entry['chunk_id']\n",
    "        if chunk_id not in chunk_id_to_text:\n",
    "            chunk_id_to_text[chunk_id] = entry['context']\n",
    "        if chunk_id not in qnas:\n",
    "            qnas[chunk_id] = []\n",
    "        qnas[chunk_id].append({'question': entry['question'], 'answer': entry['answer']})\n",
    "\n",
    "print(qnas[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafa8927-e56c-448b-b88b-f8d854c25d4d",
   "metadata": {},
   "source": [
    "# Output qna.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f26460-737f-4940-b58a-ef6caea313d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_presenter(dumper, data):\n",
    "  if len(data.splitlines()) > 1:  # check for multiline string\n",
    "    return dumper.represent_scalar('tag:yaml.org,2002:str', data, style='|')\n",
    "  elif len(data) > 80:\n",
    "    data = \"\\n\".join(wrap(data, 80))\n",
    "    return dumper.represent_scalar('tag:yaml.org,2002:str', data, style='|')\n",
    "  return dumper.represent_scalar('tag:yaml.org,2002:str', data)\n",
    "\n",
    "yaml.add_representer(str, str_presenter)\n",
    "\n",
    "# to use with safe_dump:\n",
    "yaml.representer.SafeRepresenter.add_representer(str, str_presenter)\n",
    "\n",
    "class IndentedDumper(yaml.Dumper):\n",
    "    def increase_indent(self, flow=False, indentless=False):\n",
    "        return super(IndentedDumper, self).increase_indent(flow, False)\n",
    "\n",
    "data = {'seed_examples': []}\n",
    "for chunk_id, context in chunk_id_to_text.items():\n",
    "    data['seed_examples'].append({\n",
    "        'context': context,\n",
    "        'questions_and_answers': [\n",
    "            {\n",
    "                'question': example['question'],\n",
    "                'answer': example['answer'],\n",
    "            } for example in qnas[chunk_id]\n",
    "        ]\n",
    "    })\n",
    "\n",
    "with open('qna.yml', 'w') as yaml_file:\n",
    "    yaml.dump(data, yaml_file, Dumper=IndentedDumper, default_flow_style=False, sort_keys=False, width=80)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5149dbbf-5601-4aa5-b1e9-e454ea0a4529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sdg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
