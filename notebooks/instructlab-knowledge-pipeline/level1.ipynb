{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5da70a08-1895-4d1f-8f50-93e2134b2e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# design goals:\n",
    "#\n",
    "# - understandability\n",
    "# - modularity\n",
    "# - configurability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acd026f-65bd-4393-bb40-f8aa8bd6828b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "\n",
    "WORKSPACE_DIR=\"workspaces/default\"\n",
    "# replace with pathlib\n",
    "\n",
    "# mkdir etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012dbcdf-93d7-474f-9826-77866ceb815e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversion\n",
    "\n",
    "CONVERSION_OUTPUT_DIR = f\"{WORKSPACE_DIR}/conversion\"\n",
    "# replace with pathlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafad55e-a4c0-4d6e-9da0-49519fa9bf74",
   "metadata": {},
   "source": [
    "## Chunking\n",
    "The goal of chunking for InstructLab SDG is to provide the teacher model small and logical pieces of the source document to generate data off of.\n",
    "\n",
    "In this notebook we are doing chunking with Docling[https://docling-project.github.io/docling/examples/hybrid_chunking/#hybrid-chunking].\n",
    "\n",
    "The input to this notebook is a docling JSON file created after a docling conversion, or a directory of docling JSON files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed2cff5-e1e6-46dc-bf2e-64f401423161",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install docling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fee94a0-5c6a-4b4e-b1cb-9ec1dfb050af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.document_converter import DocumentConverter\n",
    "from docling.chunking import HybridChunker, HierarchicalChunker\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9573f9e5-7170-4716-abe8-b61d91a37e3f",
   "metadata": {},
   "source": [
    "## Set the source document path\n",
    "\n",
    "Here we're going to want to set the converted.json that comes from the conversion notebook.\n",
    "\n",
    "If the conversion notebook was not run then, setting the path to the source document in any form is fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d33dd6-2128-4dd2-8220-041bebcddc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_path = Path(\"output\")\n",
    "\n",
    "files = []\n",
    "\n",
    "if doc_path.is_file():\n",
    "    files = [doc_path]\n",
    "else:\n",
    "    files = list(doc_path.rglob(\"*.json\"))\n",
    "print(f\"Docling JSON's to chunk: {files}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2482060c-a49f-4345-aa47-d54301939387",
   "metadata": {},
   "source": [
    "## Initialize the Chunker\n",
    "\n",
    "Docling provides two chunkers, the `HierarchicalChunker` and the `HybridChunker`.\n",
    "The `HierarchicalChunker` creates chunks based on the hierarchy in the Docling document\n",
    "\n",
    "The `HybridChunker` builds on the `HierarchicalChunker` and by making it tokenization aware.\n",
    "\n",
    "The `HybridChunker` has options for a `tokenizer`, the `max_tokens` in a chunk, and whether to merge undersized peer chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50df9d91-add4-46a1-a69d-0f7f9f69542e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chunker = HierarchicalChunker()\n",
    "chunker = HybridChunker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2faac31d-3356-4b76-b0e8-0b1cbac38390",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load and chunk the converted docling document\n",
    "\n",
    "Next lets convert the document we want to chunk up into a Docling Document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db983c05-4aa6-4261-9283-2adab69bfbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_chunks = []\n",
    "for file in files:\n",
    "    try:\n",
    "        doc = DocumentConverter().convert(source=file).document\n",
    "        chunk_iter = chunker.chunk(dl_doc=doc)\n",
    "        chunks = [chunker.serialize(chunk=chunk) for chunk in chunk_iter]\n",
    "        for chunk in chunks:\n",
    "            c = dict(chunk=chunk, file=file.stem)\n",
    "            all_chunks.append(c)\n",
    "    except ConversionError as e:\n",
    "        print(f\"Skipping file {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb38545-eb84-4923-8fc4-d10ed08eab26",
   "metadata": {},
   "source": [
    "## View the Chunks\n",
    "\n",
    "To view the chunks, run through the following cell. As you can see the document is broken into small pieces with metadata about the chunk based on the document's format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdf34c7-9829-43d2-bf9f-7d1d55bb6a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(all_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c4160f-7508-4c72-b28d-b56aa4975b26",
   "metadata": {},
   "source": [
    "## Save the chunks to a text file for each chunk\n",
    "\n",
    "Each chunk is saved to an individual text file in the format: `{docling-json-file-name}-{chunk #}.txt`. Having chunking in this format is important as an input to create-sdg-seed-data notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e70d576-a2bc-4274-b660-1cbe051968b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path(\"output/chunks\")\n",
    "for i, chunk in enumerate(all_chunks):\n",
    "    chunk_path = output_dir / f\"{chunk[\"file\"]}-{i}.txt\"\n",
    "    with open(chunk_path, \"w\") as file:\n",
    "        file.write(chunk[\"chunk\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65be773-8364-4dd9-b215-80cac5b8a3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a165c38-843b-4c89-a8ad-6195b998e284",
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = [\n",
    "    lambda chunk: len(str(chunk.text)) > 500\n",
    "]\n",
    "\n",
    "dataset = {}\n",
    "for doc in docs:\n",
    "    print(f\"Chunking and filtering document {doc.document.name}\")\n",
    "\n",
    "    chunks = list(chunker.chunk(dl_doc=doc.document))\n",
    "    qa_chunks = list(get_qa_chunks(doc.document.name, chunks, filters))\n",
    "    dataset[doc.document.name] = qa_chunks\n",
    "    \n",
    "    print(f\"Created dataset {doc.document.name} with {len(qa_chunks)} QA chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65ec755-e3de-40ab-bf3a-23ebb29a705d",
   "metadata": {},
   "source": [
    "# Initialize QA generator, supplying details for which model to use\n",
    "\n",
    "GenerateOptions controls which model is used for QA generation by setting generate_options.provider below. Three options are available:\n",
    "\n",
    "* LlmProviders.WATSONX for watsonx\n",
    "* LlmProviders.OPENAI for OpenAI\n",
    "* LlmProviders.OPENAI_LIKE for any model provider with OpenAI compatible APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216b50a7-889f-4432-a29b-5a8fd55177d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install docling-sdg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b702267e-f550-4bc2-bce4-c0fcecbbd292",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling_sdg.qa.generate import Generator\n",
    "from docling_sdg.qa.base import GenerateOptions, LlmProviders\n",
    "\n",
    "generate_options = GenerateOptions(api_key=\"fake\", project_id=\"project_id\")\n",
    "generate_options.provider = LlmProviders.OPENAI_LIKE\n",
    "generate_options.api_key = \"fake\"\n",
    "generate_options.model_id = \"mixtral\" # for local ollama\n",
    "generate_options.generated_file = f\"data/chunks-{filename_base}.jsonl\"\n",
    "\n",
    "gen = Generator(generate_options=generate_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57edff5-9a13-47fb-9248-9140ae5baaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc, chunks in dataset.items():\n",
    "    print(f\"processing chunks that looks like:\\n{chunks[0].text}\")\n",
    "    results = gen.generate_from_chunks(chunks)\n",
    "    print(f\"{doc}: {results.status}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea64b8f0-dd6c-4776-8646-9731433f909b",
   "metadata": {},
   "source": [
    "# Read generated QAs and restructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df2c533-30d7-4c30-9907-7c5655fd2246",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import yaml\n",
    "from textwrap import wrap\n",
    "\n",
    "qnas = {}\n",
    "chunk_id_to_text = {}\n",
    "with open(generate_options.generated_file, \"rt\") as f:\n",
    "    for line in f.readlines():\n",
    "        entry = json.loads(line)\n",
    "        chunk_id = entry['chunk_id']\n",
    "        if chunk_id not in chunk_id_to_text:\n",
    "            chunk_id_to_text[chunk_id] = entry['context']\n",
    "        if chunk_id not in qnas:\n",
    "            qnas[chunk_id] = []\n",
    "        qnas[chunk_id].append({'question': entry['question'], 'answer': entry['answer']})\n",
    "\n",
    "print(qnas[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafa8927-e56c-448b-b88b-f8d854c25d4d",
   "metadata": {},
   "source": [
    "# Output qna.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f26460-737f-4940-b58a-ef6caea313d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_presenter(dumper, data):\n",
    "  if len(data.splitlines()) > 1:  # check for multiline string\n",
    "    return dumper.represent_scalar('tag:yaml.org,2002:str', data, style='|')\n",
    "  elif len(data) > 80:\n",
    "    data = \"\\n\".join(wrap(data, 80))\n",
    "    return dumper.represent_scalar('tag:yaml.org,2002:str', data, style='|')\n",
    "  return dumper.represent_scalar('tag:yaml.org,2002:str', data)\n",
    "\n",
    "yaml.add_representer(str, str_presenter)\n",
    "\n",
    "# to use with safe_dump:\n",
    "yaml.representer.SafeRepresenter.add_representer(str, str_presenter)\n",
    "\n",
    "class IndentedDumper(yaml.Dumper):\n",
    "    def increase_indent(self, flow=False, indentless=False):\n",
    "        return super(IndentedDumper, self).increase_indent(flow, False)\n",
    "\n",
    "data = {'seed_examples': []}\n",
    "for chunk_id, context in chunk_id_to_text.items():\n",
    "    data['seed_examples'].append({\n",
    "        'context': context,\n",
    "        'questions_and_answers': [\n",
    "            {\n",
    "                'question': example['question'],\n",
    "                'answer': example['answer'],\n",
    "            } for example in qnas[chunk_id]\n",
    "        ]\n",
    "    })\n",
    "\n",
    "with open('qna.yml', 'w') as yaml_file:\n",
    "    yaml.dump(data, yaml_file, Dumper=IndentedDumper, default_flow_style=False, sort_keys=False, width=80)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5149dbbf-5601-4aa5-b1e9-e454ea0a4529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sdg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
