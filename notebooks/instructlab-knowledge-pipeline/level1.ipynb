{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5da70a08-1895-4d1f-8f50-93e2134b2e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# design goals:\n",
    "#\n",
    "# - understandability\n",
    "# - modularity\n",
    "# - configurability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0acd026f-65bd-4393-bb40-f8aa8bd6828b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "WORKSPACE_NAME = \"default\"\n",
    "\n",
    "WORKSPACE_ROOT = Path(\"workspaces\")\n",
    "WORKSPACE_ROOT.mkdir(exist_ok=True)\n",
    "\n",
    "WORKSPACE_DIR = WORKSPACE_ROOT / WORKSPACE_NAME\n",
    "WORKSPACE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "SOURCE_DOCUMENT = None # to process a specific document, set its path here; otherwise, the entire source documents repository will be used\n",
    "SOURCE_DOCUMENT_DIR = WORKSPACE_DIR / \"source_documents\"\n",
    "\n",
    "CONVERSION_OUTPUT_DIR = WORKSPACE_DIR / \"conversion\"\n",
    "CONVERSION_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CHUNKING_OUTPUT_DIR = WORKSPACE_DIR / \"chunking\"\n",
    "CHUNKING_OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "AUTHORING_OUTPUT_DIR = WORKSPACE_DIR / \"authoring\"\n",
    "AUTHORING_OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "SEED_EXAMPLE_INPUT_DIR = WORKSPACE_DIR / \"sdg_inputs\"\n",
    "SEED_EXAMPLE_INPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "SEED_EXAMPLE_OUTPUT_DIR = WORKSPACE_DIR / \"seed_examples\"\n",
    "SEED_EXAMPLE_OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "SDG_OUTPUT_DIR = WORKSPACE_DIR / \"sdg\"\n",
    "SDG_OUTPUT_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344b7ac5-fc2a-40a8-8e1f-e8dd8b1153e7",
   "metadata": {},
   "source": [
    "# Document Conversion\n",
    "\n",
    "This notebook uses [Docling](https://github.com/docling-project/docling) to convert any type of document into a Docling Document. A Docling Document is the representation of the document after conversion that can be exported as JSON. The JSON output of this notebook can then be used in others such as one that uses Docling's chunking methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b91d4b2e-19cd-46e7-a912-ba9b2904c7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq docling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f3804ef-4961-44b1-91c9-62929f422702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** WARNING! Only one file at a time is supported at this time.\n",
      "***** Using workspaces/default/source_documents/2502.01618v3.pdf)\n",
      "Files to convert: [PosixPath('workspaces/default/source_documents/2502.01618v3.pdf')]\n"
     ]
    }
   ],
   "source": [
    "files = []\n",
    "\n",
    "if SOURCE_DOCUMENT:\n",
    "    files.append(Path(SOURCE_DOCUMENT))\n",
    "else:\n",
    "    print(\"***** WARNING! Only one file at a time is supported at this time.\")\n",
    "    files = list(SOURCE_DOCUMENT_DIR.rglob(\"*.pdf\"))\n",
    "    print(f\"***** Using {files[0]})\")\n",
    "\n",
    "print(f\"Files to convert: {files}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749fb64b-d089-4844-9330-7f3639819e7a",
   "metadata": {},
   "source": [
    "Next we set the configuration options for our conversion pipeline. The PDF Conversion options set here are the defaults. More information about pipeline configuration can be found on Docling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "157c5e02-edd1-44f6-b20f-f6b4bda1aae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
    "\n",
    "pipeline_options = PdfPipelineOptions() # TODO: show the options that can be set\n",
    "\n",
    "doc_converter = DocumentConverter(\n",
    "    format_options={\n",
    "        InputFormat.PDF: PdfFormatOption(\n",
    "            pipeline_options=pipeline_options\n",
    "        )\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73400c74-dead-4998-aee2-ddb00ddaa276",
   "metadata": {},
   "source": [
    "Finally, we convert every document into Docling JSON as long as it is a valid file type to be converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a200039c-b8b2-4087-88ba-7bfb0e393cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path of JSON output is: /Users/astoyano/Documents/code/examples/notebooks/instructlab-knowledge-pipeline/workspaces/default/conversion/2502.01618v3.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "for file in files:\n",
    "    doc = doc_converter.convert(source=file).document\n",
    "    doc_dict = doc.export_to_dict()\n",
    "\n",
    "    json_output_path = CONVERSION_OUTPUT_DIR / f\"{file.stem}.json\"\n",
    "    with open(json_output_path, \"w\") as f:\n",
    "        json.dump(doc_dict, f)\n",
    "        print(f\"Path of JSON output is: {Path(json_output_path).resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafad55e-a4c0-4d6e-9da0-49519fa9bf74",
   "metadata": {},
   "source": [
    "## Chunking\n",
    "\n",
    "The goal of chunking the converted documents is to provide the teacher model small and logical pieces of the source document to generate data off of.\n",
    "\n",
    "In this notebook we are doing chunking with [Docling](https://docling-project.github.io/docling/examples/hybrid_chunking/#hybrid-chunking).\n",
    "\n",
    "The input to this notebook is a docling JSON file created after a docling conversion, or a directory of docling JSON files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2482060c-a49f-4345-aa47-d54301939387",
   "metadata": {},
   "source": [
    "## Initialize the Chunker\n",
    "\n",
    "Docling provides two chunkers, the `HierarchicalChunker` and the `HybridChunker`.\n",
    "The `HierarchicalChunker` creates chunks based on the hierarchy in the Docling document\n",
    "\n",
    "The `HybridChunker` builds on the `HierarchicalChunker` and by making it tokenization aware.\n",
    "\n",
    "The `HybridChunker` has options for a `tokenizer`, the `max_tokens` in a chunk, and whether to merge undersized peer chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50df9d91-add4-46a1-a69d-0f7f9f69542e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.chunking import HybridChunker\n",
    "\n",
    "chunker = HybridChunker() # TODO: expose configuration options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ce1d6f-b8d3-470c-b3c9-675911f0ee92",
   "metadata": {},
   "source": [
    "## Load and chunk the converted docling document\n",
    "\n",
    "Next lets convert the document we want to chunk up into a Docling Document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db983c05-4aa6-4261-9283-2adab69bfbd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1015 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 52 chunks from 2502.01618v3\n"
     ]
    }
   ],
   "source": [
    "all_chunks = []\n",
    "docs = []\n",
    "for file in files:\n",
    "    doc = DocumentConverter().convert(source=file)\n",
    "    docs.append(doc)\n",
    "    \n",
    "    chunk_iter = chunker.chunk(dl_doc=doc.document)\n",
    "    chunk_objs = list(chunk_iter)\n",
    "    chunks = [chunker.contextualize(chunk=chunk) for chunk in chunk_objs]\n",
    "\n",
    "    print(f\"Extracted {len(chunks)} chunks from {doc.document.name}\")\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        c = dict(chunk=chunk, file=file.stem)\n",
    "        all_chunks.append(c)\n",
    "\n",
    "# TODO: support multiple files save all chunks to single file for review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb38545-eb84-4923-8fc4-d10ed08eab26",
   "metadata": {},
   "source": [
    "## View the Chunks\n",
    "\n",
    "To view the chunks, run through the following cell. As you can see the document is broken into small pieces with metadata about the chunk based on the document's format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fdf34c7-9829-43d2-bf9f-7d1d55bb6a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Probabilistic Inference Approach to Inference-Time Scaling of LLMs using Particle-Based Monte Carlo Methods\n",
      "Isha Puri 1 Shivchander Sudalairaj 2 Guangxuan Xu 2 Kai Xu 2 Akash Srivastava 2 1 MIT CSAIL 2 Red Hat AI Innovation\n",
      "c\n"
     ]
    }
   ],
   "source": [
    "#print(all_chunks)\n",
    "print(chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c4160f-7508-4c72-b28d-b56aa4975b26",
   "metadata": {},
   "source": [
    "## Save the chunks to a text file for each chunk\n",
    "\n",
    "Each chunk is saved to an individual text file in the format: `{docling-json-file-name}-{chunk #}.txt`. Having chunking in this format is important as an input to create-sdg-seed-data notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e70d576-a2bc-4274-b660-1cbe051968b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, chunk in enumerate(all_chunks):\n",
    "    chunk_path = CHUNKING_OUTPUT_DIR / f\"{chunk['file']}-{i}.txt\"\n",
    "    with open(chunk_path, \"w\") as file:\n",
    "        file.write(chunk[\"chunk\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a510f8c7-8cd3-4867-8742-9f4f9cda9e9f",
   "metadata": {},
   "source": [
    "# Authoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86c48e52-cda7-48ac-84dc-0b844aed5f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install -qq docling-sdg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a165c38-843b-4c89-a8ad-6195b998e284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking and filtering document 2502.01618v3\n",
      "Created dataset 2502.01618v3 with 45 QA chunks\n"
     ]
    }
   ],
   "source": [
    "from docling_sdg.qa.utils import get_qa_chunks\n",
    "\n",
    "filters = [\n",
    "    lambda chunk: len(str(chunk.text)) > 500\n",
    "]\n",
    "\n",
    "dataset = {}\n",
    "for doc in docs:\n",
    "    print(f\"Chunking and filtering document {doc.document.name}\")\n",
    "\n",
    "    chunks = list(chunker.chunk(dl_doc=doc.document))\n",
    "    qa_chunks = list(get_qa_chunks(doc.document.name, chunk_objs, filters)) #TODO: decouple reference to chunk_objs from above)\n",
    "    dataset[doc.document.name] = qa_chunks\n",
    "    \n",
    "    print(f\"Created dataset {doc.document.name} with {len(qa_chunks)} QA chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65ec755-e3de-40ab-bf3a-23ebb29a705d",
   "metadata": {},
   "source": [
    "## Initialize QA generator, supplying details for which model to use\n",
    "\n",
    "GenerateOptions controls which model is used for QA generation by setting generate_options.provider below. Three options are available:\n",
    "\n",
    "* LlmProviders.WATSONX for watsonx\n",
    "* LlmProviders.OPENAI for OpenAI\n",
    "* LlmProviders.OPENAI_LIKE for any model provider with OpenAI compatible APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b702267e-f550-4bc2-bce4-c0fcecbbd292",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling_sdg.qa.generate import Generator\n",
    "from docling_sdg.qa.base import GenerateOptions, LlmProvider\n",
    "from pydantic import SecretStr\n",
    "\n",
    "generate_options = GenerateOptions(api_key=\"fake\", project_id=\"project_id\")\n",
    "generate_options.provider = LlmProvider.OPENAI_LIKE\n",
    "generate_options.api_key = SecretStr(\"fake\")\n",
    "generate_options.model_id = \"granite3.3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919199c0-3747-409a-85ab-0155ef3ebe9d",
   "metadata": {},
   "source": [
    "## Configure subset selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1197d4e-8354-45e3-9ec9-85c78ba36548",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CHUNKS_TO_SELECT_FOR_AUTHORING = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2421d07-3e6c-4355-95f4-da8e157557c7",
   "metadata": {},
   "source": [
    "## Run QA generation on selected chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e57edff5-9a13-47fb-9248-9140ae5baaca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing chunks that looks like:\n",
      "Large language models (LLMs) have achieved significant performance gains via scaling up model sizes and/or data. However, recent evidence suggests diminishing returns from such approaches, motivating scaling the computation spent at inference time. Existing inference-time scaling methods, usually with reward models, cast the task as a search problem, which tends to be vulnerable to reward hacking as a consequence of approximation errors in reward models. In this paper, we instead cast inference-time scaling as a probabilistic inference task and leverage sampling-based techniques to explore the typical set of the state distribution of a state-space model with an approximate likelihood, rather than optimize for its mode directly. We propose a novel inference-time scaling approach by adapting particle-based Monte Carlo methods to this task. Our empirical evaluation demonstrates that our methods have a 4-16x better scaling rate over our deterministic search counterparts on various challenging mathematical reasoning tasks. Using our approach, we show that Qwen2.5-Math-1.5B-Instruct can surpass GPT4o accuracy in only 4 rollouts, while Qwen2.5Math-7B-Instruct scales to o1 level accuracy in only 32 rollouts. Our work not only presents an effective method to inference-time scaling, but also connects the rich literature in probabilistic inference with inference-time scaling of LLMs to develop more robust algorithms in future work. Code, videos, and further information available at probabilistic-inference-scaling.github.io/ .\n",
      "Selected 5 contexts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:25<00:00,  5.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2502.01618v3: Status.SUCCESS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random #TODO: replace random sampling with subset selection\n",
    "\n",
    "gen = Generator(generate_options=generate_options)\n",
    "for doc, chunks in dataset.items():\n",
    "    generate_options.generated_file = AUTHORING_OUTPUT_DIR / f\"qagen-{doc}.json\"\n",
    "    \n",
    "    print(f\"processing chunks that looks like:\\n{chunks[0].text}\")\n",
    "    selected_chunks = random.sample(chunks, NUM_CHUNKS_TO_SELECT_FOR_AUTHORING)\n",
    "    print(f\"Selected {len(selected_chunks)} contexts\")\n",
    "    \n",
    "    results = gen.generate_from_chunks(selected_chunks) # automatically saves to file\n",
    "    \n",
    "    print(f\"{doc}: {results.status}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea64b8f0-dd6c-4776-8646-9731433f909b",
   "metadata": {},
   "source": [
    "## Read generated QAs and restructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9df2c533-30d7-4c30-9907-7c5655fd2246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'question': 'What type of methods does the paper propose for inference-time scaling?', 'answer': 'The paper proposes adapting particle-based Monte Carlo methods for inference-time scaling.'}, {'question': 'How do the proposed methods in this paper compare to existing inference-time scaling methods in terms of performance?', 'answer': 'The empirical evaluation demonstrates that the novel approach has a 4-16x better scaling rate over deterministic search counterparts on various challenging mathematical reasoning tasks.'}, {'question': 'Why does the paper argue for shifting focus from scaling model sizes and data to scaling computation at inference time?', 'answer': 'Recent evidence suggests diminishing returns from scaling up model sizes and/or data, motivating the authors to explore scaling the computation spent at inference time as an alternative approach.'}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import yaml\n",
    "from textwrap import wrap\n",
    "\n",
    "qnas = {}\n",
    "chunk_id_to_text = {}\n",
    "with open(generate_options.generated_file, \"rt\") as f:\n",
    "    for line in f.readlines():\n",
    "        entry = json.loads(line)\n",
    "        chunk_id = entry['chunk_id']\n",
    "        if chunk_id not in chunk_id_to_text:\n",
    "            chunk_id_to_text[chunk_id] = entry['context']\n",
    "        if chunk_id not in qnas:\n",
    "            qnas[chunk_id] = []\n",
    "        qnas[chunk_id].append({'question': entry['question'], 'answer': entry['answer']})\n",
    "\n",
    "print(list(qnas.values())[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6d6c26-f4d5-420d-ae78-ac28cf39efd3",
   "metadata": {},
   "source": [
    "## Define metadata for qna.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7130e90-2b65-4008-86f7-194da74a9523",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCUMENT_OUTLINE = \"A Probabilistic Inference Approach to Inference-Time Scaling of LLMs using Particle-Based Monte Carlo Methods\"\n",
    "DOMAIN = \"artificial intelligence\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafa8927-e56c-448b-b88b-f8d854c25d4d",
   "metadata": {},
   "source": [
    "## Output qna.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7f26460-737f-4940-b58a-ef6caea313d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "qna_output_path = AUTHORING_OUTPUT_DIR / \"qna.yaml\"\n",
    "\n",
    "def str_presenter(dumper, data):\n",
    "  if len(data.splitlines()) > 1:  # check for multiline string\n",
    "    return dumper.represent_scalar('tag:yaml.org,2002:str', data, style='|')\n",
    "  elif len(data) > 80:\n",
    "    data = \"\\n\".join(wrap(data, 80))\n",
    "    return dumper.represent_scalar('tag:yaml.org,2002:str', data, style='|')\n",
    "  return dumper.represent_scalar('tag:yaml.org,2002:str', data)\n",
    "\n",
    "yaml.add_representer(str, str_presenter)\n",
    "\n",
    "# to use with safe_dump:\n",
    "yaml.representer.SafeRepresenter.add_representer(str, str_presenter)\n",
    "\n",
    "class IndentedDumper(yaml.Dumper):\n",
    "    def increase_indent(self, flow=False, indentless=False):\n",
    "        return super(IndentedDumper, self).increase_indent(flow, False)\n",
    "\n",
    "data = {'seed_examples': []}\n",
    "for chunk_id, context in chunk_id_to_text.items():\n",
    "    data['seed_examples'].append({\n",
    "        'context': context,\n",
    "        'questions_and_answers': [\n",
    "            {\n",
    "                'question': example['question'],\n",
    "                'answer': example['answer'],\n",
    "            } for example in qnas[chunk_id]\n",
    "        ]\n",
    "    })\n",
    "\n",
    "data['document_outline'] = DOCUMENT_OUTLINE\n",
    "d\n",
    "\n",
    "with open(qna_output_path, 'w') as yaml_file:\n",
    "    yaml.dump(data, yaml_file, Dumper=IndentedDumper, default_flow_style=False, sort_keys=False, width=80)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9ea149-844b-4330-90ec-d0ca7ab12b90",
   "metadata": {},
   "source": [
    "## View generated qna.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1293d445-b826-4b92-ad20-9b121ac60e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed_examples:\n",
      "  - context: |-\n",
      "      Large language models (LLMs) have achieved significant performance gains via\n",
      "      scaling up model sizes and/or data. However, recent evidence suggests\n",
      "      diminishing returns from such approaches, motivating scaling the computation\n",
      "      spent at inference time. Existing inference-time scaling methods, usually with\n",
      "      reward models, cast the task as a search problem, which tends to be vulnerable\n",
      "      to reward hacking as a consequence of approximation errors in reward models. In\n",
      "      this paper, we instead cast inference-time scaling as a probabilistic inference\n",
      "      task and leverage sampling-based techniques to explore the typical set of the\n",
      "      state distribution of a state-space model with an approximate likelihood, rather\n",
      "      than optimize for its mode directly. We propose a novel inference-time scaling\n",
      "      approach by adapting particle-based Monte Carlo methods to this task. Our\n",
      "      empirical evaluation demonstrates that our methods have a 4-16x better scaling\n",
      "      rate over our deterministic search counterparts on various challenging\n",
      "      mathematical reasoning tasks. Using our approach, we show that\n",
      "      Qwen2.5-Math-1.5B-Instruct can surpass GPT4o accuracy in only 4 rollouts, while\n",
      "      Qwen2.5Math-7B-Instruct scales to o1 level accuracy in only 32 rollouts. Our\n",
      "      work not only presents an effective method to inference-time scaling, but also\n",
      "      connects the rich literature in probabilistic inference with inference-time\n",
      "      scaling of LLMs to develop more robust algorithms in future work. Code, videos,\n",
      "      and further information available at probabilistic-inference-scaling.github.io/\n",
      "      .\n",
      "    questions_and_answers:\n",
      "      - question: What type of methods does the paper propose for inference-time scaling?\n",
      "        answer: |-\n",
      "          The paper proposes adapting particle-based Monte Carlo methods for inference-\n",
      "          time scaling.\n",
      "      - question: |-\n",
      "          How do the proposed methods in this paper compare to existing inference-time\n",
      "          scaling methods in terms of performance?\n",
      "        answer: |-\n",
      "          The empirical evaluation demonstrates that the novel approach has a 4-16x better\n",
      "          scaling rate over deterministic search counterparts on various challenging\n",
      "          mathematical reasoning tasks.\n",
      "      - question: |-\n",
      "          Why does the paper argue for shifting focus from scaling model sizes and data to\n",
      "          scaling computation at inference time?\n",
      "        answer: |-\n",
      "          Recent evidence suggests diminishing returns from scaling up model sizes and/or\n",
      "          data, motivating the authors to explore scaling the computation spent at\n",
      "          inference time as an alternative approach.\n",
      "  - context: |-\n",
      "      Large language models (LLMs) have demonstrated remarkable improvements in performance through scaling up\n",
      "      1 MIT CSAIL 2 Red Hat AI Innovation. Correspondence to: Isha Puri < ishapuri@mit.edu > , Shivchander Sudalairaj < ssudalai@redhat.com > .\n",
      "      Copyright 2025 by the author(s).\n",
      "      Figure 1. State-space model for inference-time scaling. c is a prompt, x , . . . , x 1 T are sequence of partial LLM outputs and o , . . . , o 1 T are the 'observed' acceptance. We cast inferencetime scaling as to estimate the latent states conditioned on o t = 1 for t = 1 2 , , . . . , T , i.e. all being accepted.\n",
      "      model sizes and/or data. While frontier models have relied heavily on larger datasets and an ever-increasing number of learnable parameters (Kaplan et al., 2020; Snell et al., 2024), smaller LLMs have successfully leveraged domain-specific data to match the performance of larger, general-purpose models (Sudalairaj et al., 2024; Pareja et al., 2024). However, recent reports indicate plateaus in performance gains through such scaling methods. Consequently, inferencetime (aka compute-time / test-time) scaling has emerged as a promising alternative to improve model performance (Beeching et al., 2024). Proprietary models like OpenAI's o1 (OpenAI et al., 2024) and o3 have demonstrated the benefits of allocating more computation resources at inference time, particularly for complex reasoning and math tasks. These inference-time scaling techniques not only enhance model capability but also allow smaller models to achieve performance levels comparable to their larger counterparts, making advanced AI more accessible for low-resource devices.\n",
      "      Recent work (Lightman et al., 2023a) has framed inferencetime scaling as a search problem guided by a process reward model (PRM). This perspective has led to the successful application of classic algorithms such as best-of-n (BoN; Brown et al., 2024), beam search (Zhou et al., 2024; Snell et al., 2024), and Monte Carlo tree search (MCTS; Guan et al., 2025), which refine model outputs by systematically exploring a broader search space. This process is sometimes\n",
      "    questions_and_answers:\n",
      "      - question: |-\n",
      "          What is the name of the process that frames inference-time scaling as a search\n",
      "          problem?\n",
      "        answer: Process Reward Model (PRM)\n",
      "      - question: |-\n",
      "          How do recent works apply classic algorithms to improve model outputs during\n",
      "          inference-time scaling?\n",
      "        answer: |-\n",
      "          Recent works apply classic algorithms such as best-of-n (BoN), beam search, and\n",
      "          Monte Carlo tree search (MCTS) to refine model outputs by systematically\n",
      "          exploring a broader search space.\n",
      "  - context: |-\n",
      "      (a) Llama-3.2-1B-Instruct\n",
      "      (b) Llama-3.1-8B-Instruct\n",
      "      (c) Qwen2.5-Math-1.5B-Instruct\n",
      "      (d) Qwen2.5-Math-7B-Instruct\n",
      "      Figure 2. Performance of PF compared to other inference-time scaling methods across different model families. Figure 2a and Figure 2b demonstrate results for the Llama-3 family, where PF outperforms WBoN and DVTS in both cases and approaches the performance of much larger models like Llama-3.1-70B and even GPT-4o . Figure 2c and Figure 2d show results for the Qwen family, where PF achieves superior scaling against baslines, enabling the smaller model Qwen2.5-Math-1.5B-Instruct to surpass GPT-4o in performance within a limited compute budget. Larger Qwen2.5-Math-7B-Instruct model efficiently scale to match o1-preview performance on MATH500.\n",
      "      referred to as 'thinking/reasoning'.\n",
      "      However, we argue that a search-based formulation becomes problematic when the reward model is imperfect-an inherent issue since these models are only approximations of an unknown true classification or preference function. Empirically, this often leads to reward hacking, where the final output is optimized to score well according to the reward model but fails to be useful and/or correct (Snell et al., 2024).\n",
      "      The idea of using more computation to refine results is a fundamental feature of many classic probabilistic inference methods. For instance, Markov chain Monte Carlo (MCMC) methods improve inference asymptotically with more iterations, while particle-based Monte Carlo methods enhance accuracy as the number of particles increases.\n",
      "    questions_and_answers:\n",
      "      - question: |-\n",
      "          Which Llama model version outperforms WBoN and DVTS in both cases according to\n",
      "          Figure 2a and Figure 2b?\n",
      "        answer: Llama-3.2-1B-Instruct\n",
      "      - question: |-\n",
      "          How do the performance scaling methods compare between Llama and Qwen families\n",
      "          as per the figures?\n",
      "        answer: |-\n",
      "          PF outperforms baselines in both Llama and Qwen families, with\n",
      "          Llama-3.2-1B-Instruct surpassing WBoN and DVTS, and Qwen2.5-Math-7B-Instruct\n",
      "          matching o1-preview performance on MATH500 within a limited compute budget.\n",
      "      - question: |-\n",
      "          Given the issue of reward hacking in search-based formulations with imperfect\n",
      "          reward models, why does the author suggest using more computation to refine\n",
      "          results as seen in classic probabilistic inference methods?\n",
      "        answer: |-\n",
      "          The author suggests this because it's a fundamental feature of many classic\n",
      "          probabilistic inference methods like MCMC and particle-based Monte Carlo, which\n",
      "          improve accuracy with increased iterations or particles, offering an alternative\n",
      "          to address reward hacking issues.\n",
      "  - context: \"\\xB7 Propagation: We start by propagating the set of particles S t -1\\\n",
      "      \\ via initialization ( t = 1 ) or transition ( t > 1 ) and calculate their weights.\\\n",
      "      \\ This produces a set of weighted particles S \\u2032 t = { x ( ) i t , w ( )\\\n",
      "      \\ i t } , which represents partial generation upto step t and their importance;\\n\\\n",
      "      \\xB7 Resampling: We sample with replacement over the particles to produce a\\\n",
      "      \\ new set of particles with the same number. Specifically, let the resampling\\\n",
      "      \\ distribution (over index j ) be\\n<!-- formula-not-decoded -->\\nWe sample {\\\n",
      "      \\ j ( ) i t \\u223C P t ( j = ) i } M i =1 and obtain a new set of particles\\\n",
      "      \\ S t = { x j ( ) i t t , w j ( ) i t t } . This step is essentially a probabilistic\\\n",
      "      \\ search with higher chances to explore high reward partial generations: These\\\n",
      "      \\ weights do not blindly guide the selection of high-reward particles at every\\\n",
      "      \\ stage of the search-they retain a degree of stochasticity that encourages\\\n",
      "      \\ exploration of under-explored regions of the sample space-explorations that\\\n",
      "      \\ may discover higher value answers later on.\\nNote that the resampling step\\\n",
      "      \\ in particle filtering maintains a natural balance between exploiting promising\\\n",
      "      \\ hypotheses and exploring less-certain regions that may yield novel solutions.\\\n",
      "      \\ By maintaining a diverse population of particles and dynamically adjusting\\\n",
      "      \\ their weights at each step, our method allows a level of flexibility that\\\n",
      "      \\ is absent in traditional strategies, such as greedy search or beam search.\\\n",
      "      \\ In general, the ability to guide exploration using PRM-based scores allows\\\n",
      "      \\ the framework to harness the strengths of reward models without being limited\\\n",
      "      \\ by their flaws.\\nImportantly, this approach ensures that inference scaling\\\n",
      "      \\ remains fruitful within smaller compute budgets, as the resampling and unrolling\\\n",
      "      \\ operations are computationally efficient and can be parallelized across particles.\\\n",
      "      \\ With proper prefix caching, the total computation on generation is as much\\\n",
      "      \\ as that for generating N complete answers directly.\\nFigure 3 provides an\\\n",
      "      \\ illustration of the method with 4 particles with comparison to beam search,\\\n",
      "      \\ and the overall algorithm is detailed in Algorithm 1.\"\n",
      "    questions_and_answers:\n",
      "      - question: |-\n",
      "          What is the initial step after propagating particles via initialization or\n",
      "          transition?\n",
      "        answer: |-\n",
      "          The initial step after propagating particles is calculating their weights, which\n",
      "          produces a set of weighted particles representing partial generation up to step\n",
      "          t and their importance.\n",
      "      - question: How does the resampling process work in particle filtering?\n",
      "        answer: \"In resampling, we sample with replacement over the particles to produce\\\n",
      "          \\ a new\\nset of particles with the same number. This is done by sampling\\\n",
      "          \\ { j ( ) i t \\u223C P\\nt ( j = ) i } M i =1 and obtaining a new set of\\\n",
      "          \\ particles S t = { x j ( ) i t ,\\nw j ( ) i t } . This step maintains a\\\n",
      "          \\ balance between exploiting promising\\nhypotheses and exploring less-certain\\\n",
      "          \\ regions, ensuring flexibility in\\nexploration.\"\n",
      "      - question: |-\n",
      "          How does the resampling step in particle filtering contribute to the overall\n",
      "          inference process?\n",
      "        answer: |-\n",
      "          The resampling step ensures that inference scaling remains fruitful within\n",
      "          smaller compute budgets by maintaining a diverse population of particles and\n",
      "          dynamically adjusting their weights at each step. This approach allows for\n",
      "          efficient computation through parallelization across particles, making it more\n",
      "          effective than traditional strategies like greedy search or beam search.\n",
      "  - context: \"\\xB7 Pass@1: single greedy generation from the model, serving as the\\\n",
      "      \\ 'bottom-line' performance.\\n\\xB7 BoN/WBoN (Brown et al., 2024): (weighted)\\\n",
      "      \\ best-of-N is the most straightforward inference-time scaling method using\\\n",
      "      \\ reward models.\\n\\xB7 DVTS (Beeching et al., 2024): a parallel extension of\\\n",
      "      \\ beam search that improves the exploration hence overall scaling performance.\\\n",
      "      \\ 1\\nDatasets To evaluate our methods and baselines, we consider widely-used\\\n",
      "      \\ datasets spanning multiple domains and difficulty levels and challenging benchmarks,\\\n",
      "      \\ ensuring a robust assessment of the methods' performance across basic and\\\n",
      "      \\ advanced problem-solving and reasoning tasks.\\n\\xB7 MATH500 (Lightman et al.,\\\n",
      "      \\ 2023b): A dataset containing 500 high-difficulty competition-level problems\\\n",
      "      \\ from various mathematical domains.\\n\\xB7 AIME2024 (AI-MO, 2023): A collection\\\n",
      "      \\ of 30 problems from the American Invitational Mathematics Examination (AIME\\\n",
      "      \\ I and II) 2024.\\nParsing and scoring To evaluate model-generated responses,\\\n",
      "      \\ we enforce a structured answer format using a system prompt (see Appendix\\\n",
      "      \\ A.2). This prompt ensures that the final answer is enclosed within a \\\\ boxed\\\n",
      "      \\ {} expression, facilitating automated extraction. We provide a detailed version\\\n",
      "      \\ of our scoring process in Appendix A.3.\"\n",
      "    questions_and_answers:\n",
      "      - question: |-\n",
      "          What method is described as the 'bottom-line' performance for single greedy\n",
      "          generation from the model?\n",
      "        answer: Pass@1\n",
      "      - question: |-\n",
      "          Which methods are used to evaluate the performance of scaling methods across\n",
      "          various domains and difficulty levels?\n",
      "        answer: |-\n",
      "          Widely-used datasets spanning multiple domains and challenging benchmarks,\n",
      "          including MATH500 and AIME2024, are considered for evaluation.\n",
      "      - question: |-\n",
      "          How does DVTS improve the exploration performance compared to Pass@1 and\n",
      "          BoN/WBoN methods?\n",
      "        answer: |-\n",
      "          DVTS improves exploration performance by being a parallel extension of beam\n",
      "          search, which potentially leads to better scaling performance than single greedy\n",
      "          generation (Pass@1) or straightforward inference-time scaling methods like\n",
      "          BoN/WBoN.\n",
      "  - context: |-\n",
      "      B., Paola, B. D., Paranjape, B., Liu, B., Wu, B., Ni, B., Hancock, B., Wasti,\n",
      "      B., Spence, B., Stojkovic, B., Gamido, B., Montalvo, B., Parker, C., Burton, C.,\n",
      "      Mejia, C., Liu, C., Wang, C., Kim, C., Zhou, C., Hu, C., Chu, C.-H., Cai, C.,\n",
      "      Tindal, C., Feichtenhofer, C., Gao, C., Civin, D., Beaty, D., Kreymer, D., Li,\n",
      "      D., Adkins, D., Xu, D., Testuggine, D., David, D., Parikh, D., Liskovich, D.,\n",
      "      Foss, D., Wang, D., Le, D., Holland, D., Dowling, E., Jamil, E., Montgomery, E.,\n",
      "      Presani, E., Hahn, E., Wood, E., Le, E.-T., Brinkman, E., Arcaute, E., Dunbar,\n",
      "      E., Smothers, E., Sun, F., Kreuk, F., Tian, F., Kokkinos, F., Ozgenel, F.,\n",
      "      Caggioni, F., Kanayet, F., Seide, F., Florez, G. M., Schwarz, G., Badeer, G.,\n",
      "      Swee, G., Halpern, G., Herman, G., Sizov, G., Guangyi, Zhang, Lakshminarayanan,\n",
      "      G., Inan, H., Shojanazeri, H., Zou, H., Wang, H., Zha, H., Habeeb, H., Rudolph,\n",
      "      H., Suk, H., Aspegren, H., Goldman, H., Zhan, H., Damlaj, I., Molybog, I.,\n",
      "      Tufanov, I., Leontiadis, I., Veliche, I.-E.,\n",
      "    questions_and_answers:\n",
      "      - question: |-\n",
      "          If all authors were to collaborate on a research project, which field of study\n",
      "          do you think their combined expertise might align with, based on their diverse\n",
      "          geographical backgrounds?\n",
      "        answer: |-\n",
      "          Given the wide array of countries represented by the authors' last names, it can\n",
      "          be inferred that their collective expertise spans multiple disciplines. However,\n",
      "          considering the presence of medical terms and a seemingly clinical context,\n",
      "          their collaboration might align with the field of Medicine or Health Sciences.\n",
      "  - context: |-\n",
      "      ```\n",
      "      Evaluation System Prompt Solve the following math problem efficiently and clearly: - For simple problems (2 steps or fewer): Provide a concise solution with minimal explanation. - For complex problems (3 steps or more): Use this step-by-step format: ## Step 1: [Concise description] [Brief explanation and calculations] ## Step 2: [Concise description] [Brief explanation and calculations] Regardless of the approach, always conclude with: Therefore, the final answer is: $\\boxed{answer}$. I hope it is correct. Where [answer] is just the final number or expression that solves the problem.\n",
      "      ```\n",
      "      ```\n",
      "      PRM Input Format ## Step 1: [Concise description] [Brief explanation and calculations] <reward_token> ## Step 2: [Concise description] [Brief explanation and calculations] <reward_token> ## Step 3: [Concise description] [Brief explanation and calculations] <reward_token>\n",
      "      ```\n",
      "    questions_and_answers:\n",
      "      - question: |-\n",
      "          What is the format for providing a concise solution with minimal explanation for\n",
      "          simple problems?\n",
      "        answer: \"The format is to provide a concise solution directly followed by\\\n",
      "          \\ 'Therefore, the\\nfinal answer is: $\\boxed{answer}$. I hope it is correct.'\"\n",
      "      - question: |-\n",
      "          How many steps are required in the step-by-step format for complex problems\n",
      "          according to the given instructions?\n",
      "        answer: |-\n",
      "          Three steps are required in the step-by-step format for complex problems, as\n",
      "          indicated by '## Step 1:', '## Step 2:', and '## Step 3:'\n",
      "      - question: |-\n",
      "          Why does the provided system specify different formats for simple versus complex\n",
      "          math problems?\n",
      "        answer: |-\n",
      "          The system specifies different formats to accommodate varying problem\n",
      "          complexities, ensuring concise solutions for simpler issues while providing\n",
      "          detailed step-by-step guidance for more intricate mathematical problems.\n",
      "  - context: \"K., Chiu, K., Bhalla, K., Lakhotia, K., Rantala-Yeary, L., van der Maaten,\\\n",
      "      \\ L.,\\nChen, L., Tan, L., Jenkins, L., Martin, L., Madaan, L., Malo, L., Blecher,\\\n",
      "      \\ L.,\\nLandzaat, L., de Oliveira, L., Muzzi, M., Pasupuleti, M., Singh, M.,\\\n",
      "      \\ Paluri, M.,\\nKardas, M., Tsimpoukelli, M., Oldham, M., Rita, M., Pavlova,\\\n",
      "      \\ M., Kambadur, M.,\\nLewis, M., Si, M., Singh, M. K., Hassan, M., Goyal, N.,\\\n",
      "      \\ Torabi, N., Bashlykov,\\nN., Bogoychev, N., Chatterji, N., Zhang, N., Duchenne,\\\n",
      "      \\ O., Celebi, O., Alrassy,\\nP., Zhang, P., Li, P., \\xB8 Vasic, P., Weng, P.,\\\n",
      "      \\ Bhargava, P., Dubal, P., Krishnan,\\nP., Koura, P. S., Xu, P., He, Q., Dong,\\\n",
      "      \\ Q., Srinivasan, R., Ganapathy, R.,\\nCalderer, R., Cabral, R. S., Stojnic,\\\n",
      "      \\ R., Raileanu, R., Maheswari, R., Girdhar,\\nR., Patel, R., Sauvestre, R., Polidoro,\\\n",
      "      \\ R., Sumbaly, R., Taylor, R., Silva, R.,\\nHou, R., Wang, R., Hosseini, S.,\\\n",
      "      \\ Chennabasappa, S., Singh, S., Bell, S., Kim, S.\\nS., Edunov, S., Nie, S.,\\\n",
      "      \\ Narang, S., Raparthy, S., Shen, S., Wan, S., Bhosale,\\nS., Zhang, S., Vandenhende,\\\n",
      "      \\ S., Batra, S., Whitman, S.,\"\n",
      "    questions_and_answers:\n",
      "      - question: |-\n",
      "          If we assume that all listed individuals are part of a single research project\n",
      "          or team, what can we infer about their collaboration?\n",
      "        answer: |-\n",
      "          Given the sheer number (over 60) and diverse last names, it can be inferred that\n",
      "          these individuals likely represent a large, international, multi-disciplinary\n",
      "          group possibly working on a complex, extensive research project or initiative.\n",
      "  - context: |-\n",
      "      Particle-based Monte Carlo methods are powerful tools for probabilistic\n",
      "      inference. Sequential Monte Carlo (Moral, 1997) or particle filtering (Swendsen\n",
      "      & Wang, 1986) has been the classical way to approximate complex posterior\n",
      "      distributions over state-space models. Particle Gibbs (PG) sampling (Andrieu et\n",
      "      al., 2010) extends these approaches by integrating MCMC techniques for improved\n",
      "      inference. (Lew et al., 2023) and (Loula et al., 2025) introduce a probabilistic\n",
      "      programming language that applies SMC methods to steer/constrain LLM generation.\n",
      "      (Zhao et al., 2024) and (Feng et al., 2024) introduce Twisted SMC methods for\n",
      "      inference in language models.\n",
      "    questions_and_answers:\n",
      "      - question: |-\n",
      "          Who introduced a probabilistic programming language that applies SMC methods to\n",
      "          steer/constrain LLM generation?\n",
      "        answer: (Lew et al., 2023) and (Loula et al., 2025)\n",
      "  - context: \"AI-MO. Aimo validation aime dataset. https: //huggingface.co/datasets/AI-MO/\\\n",
      "      \\ aimo-validation-aime , 2023. Accessed:\\n2025-01-24.\\nAndrieu, C., Doucet,\\\n",
      "      \\ A., and Holenstein, R. Particle Markov Chain Monte Carlo Methods. Journal\\\n",
      "      \\ of the Royal Statistical Society Series B: Statistical Methodology , 72(3):\\\n",
      "      \\ 269-342, June 2010. ISSN 1369-7412, 1467-9868. doi: 10.1111/j.1467-9868.2009.00736.x.\\n\\\n",
      "      Beeching, E., Tunstall, L., and Rush, S. Scaling test-time compute with open\\\n",
      "      \\ models, 2024. URL https:// huggingface.co/spaces/HuggingFaceH4/ blogpost-scaling-test-time-compute\\\n",
      "      \\ .\\nBrown, B., Juravsky, J., Ehrlich, R., Clark, R., Le, Q. V., R\\xB4 e, C.,\\\n",
      "      \\ and Mirhoseini, A. Large Language Monkeys: Scaling Inference Compute with\\\n",
      "      \\ Repeated Sampling, July 2024.\\nCui, G., Yuan, L., Wang, Z., Wang, H., Li,\\\n",
      "      \\ W., He, B., Fan, Y., Yu, T., Xu, Q., Chen, W., Yuan, J., Chen, H., Zhang,\\\n",
      "      \\ K., Lv, X., Wang, S., Yao, Y., Peng, H., Cheng, Y ., Liu, Z., Sun, M., Zhou,\\\n",
      "      \\ B., and Ding, N. Process reinforcement through implicit rewards, 2025.\\nFeng,\\\n",
      "      \\ S., Kong, X., Ma, S., Zhang, A., Yin, D., Wang, C., Pang, R., and Yang, Y.\\\n",
      "      \\ Step-by-step reasoning for math problems via twisted sequential monte carlo,\\\n",
      "      \\ 2024. URL https://arxiv.org/abs/2410.01920 .\"\n",
      "    questions_and_answers:\n",
      "      - question: |-\n",
      "          What sources provide information about particle Markov Chain Monte Carlo Methods\n",
      "          and scaling test-time compute with open models?\n",
      "        answer: |-\n",
      "          The Royal Statistical Society Series B: Statistical Methodology and\n",
      "          HuggingFaceH4's blogpost-scaling-test-time-compute, respectively.\n",
      "      - question: |-\n",
      "          Based on the provided context, what can be inferred about the relationship\n",
      "          between large language models and inference compute scaling?\n",
      "        answer: |-\n",
      "          The context suggests that large language models require significant inference\n",
      "          compute resources for efficient operation, as discussed in works like 'Large\n",
      "          Language Monkeys: Scaling Inference Compute with Repeated Sampling' by Brown et\n",
      "          al. (2024) and approaches such as step-by-step reasoning via Twisted Sequential\n",
      "          Monte Carlo (Feng et al., 2024).\n",
      "  - context: \"Input : same as Algorithm 1 with the number of Gibbs iterations T\\n\\\n",
      "      ```\\nRun Algorithm 1 to get a set of particles { x ( ) i 1: t } N i =1 for j\\\n",
      "      \\ = 1 , . . . , T do Compute rewards w = [\\u02C6( r x (1) 1: t ) , . . . , r\\\n",
      "      \\ \\u02C6( x ( N ) 1: t )] Compute softmax distribution \\u03B8 = softmax( w )\\\n",
      "      \\ Sample reference particle x ref 1: t := x ( j ) 1: t where j \\u223C P ( j\\\n",
      "      \\ = ) = i \\u03B8 Initialize N -1 particles { x ( ) i 1 \\u223C p M ( \\xB7 | c\\\n",
      "      \\ ) } N -1 i =1 t \\u2190 1 while not all particles stop do Update w = [\\u02C6\\\n",
      "      ( r x (1) 1: t ) , . . . , r \\u02C6( x ( N -1) 1: t ) , r \\u02C6( x ref 1: t\\\n",
      "      \\ )] Compute softmax distribution \\u03B8 = softmax( w ) Sample indices { j (\\\n",
      "      \\ ) i t } N i =1 \\u223C P t ( j = ) = i \\u03B8 i Update the set of particles\\\n",
      "      \\ as { x ( j ( ) i t ) 1: t } N i =1 Transition { x ( ) i t +1 \\u223C p M (\\\n",
      "      \\ \\xB7 | c, x ( ) i t +1 ) } N i =1 t \\u2190 t +1 end while end for Return :\\\n",
      "      \\ the set of particles in the end\\n```\"\n",
      "    questions_and_answers:\n",
      "      - question: What is the initial input required for the algorithm described?\n",
      "        answer: |-\n",
      "          The initial input required for the algorithm is a set of particles { x (i) 1:t }\n",
      "          N i=1 obtained from running Algorithm 1.\n",
      "      - question: |-\n",
      "          If the algorithm stops when all particles have stopped, what does this imply\n",
      "          about the behavior of the system it models?\n",
      "        answer: |-\n",
      "          The fact that the algorithm stops when all particles have stopped implies that\n",
      "          under the current model and parameters, there is no further significant movement\n",
      "          or change in state being simulated by these particles.\n",
      "  - context: |-\n",
      "      Li, W., Wang, W., Jiang, W., Bouaziz, W., Constable, W., Tang, X., Wu, X., Wang, X., Wu, X., Gao, X., Kleinman, Y., Chen, Y., Hu, Y., Jia, Y., Qi, Y., Li, Y ., Zhang, Y ., Zhang, Y ., Adi, Y ., Nam, Y ., Yu, Wang, Zhao, Y., Hao, Y., Qian, Y., Li, Y., He, Y., Rait, Z., DeVito, Z., Rosnbrick, Z., Wen, Z., Yang, Z., Zhao, Z., and Ma, Z. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783 .\n",
      "      Guan, X., Zhang, L. L., Liu, Y., Shang, N., Sun, Y., Zhu, Y., Yang, F., and Yang, M. rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking, January 2025.\n",
      "      Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. Scaling laws for neural language models, 2020. URL https://arxiv.org/abs/2001. 08361 .\n",
      "      Lew, A. K., Zhi-Xuan, T., Grand, G., and Mansinghka, V. K. Sequential monte carlo steering of large language models using probabilistic programs, 2023. URL https: //arxiv.org/abs/2306.03081 .\n",
      "      Lightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J., Sutskever, I., and Cobbe, K. Let's Verify Step by Step, May 2023a.\n",
      "    questions_and_answers:\n",
      "      - question: Who is the author of the paper titled 'The llama 3 herd of models'?\n",
      "        answer: |-\n",
      "          Li, W., Wang, W., Jiang, W., Bouaziz, W., Constable, W., Tang, X., Wu, X., Wang,\n",
      "          X., Wu, X., Gao, X., Kleinman, Y., Chen, Y., Hu, Y., Jia, Y., Qi, Y., Li, Y.,\n",
      "          Zhang, Y., Zhang, Y., Adi, Y., Nam, Y., Yu, Wang, Zhao, Y., Hao, Y., Qian, Y.,\n",
      "          Li, Y., He, Y., Rait, Z., DeVito, Z., Rosnbrick, Z., Wen, Z., Yang, Z., Zhao,\n",
      "          Z., and Ma, Z.\n",
      "      - question: What are the main topics covered in the provided papers?\n",
      "        answer: |-\n",
      "          The papers cover topics such as 'The llama 3 herd of models' (LLM), a new set of\n",
      "          language models; rStar-Math, a method for small language models to master math\n",
      "          reasoning using self-evolved deep thinking; scaling laws for neural language\n",
      "          models; sequential monte carlo steering of large language models using\n",
      "          probabilistic programs; and Let's Verify Step by Step, an approach for model\n",
      "          verification through step-by-step execution.\n",
      "      - question: |-\n",
      "          How does the concept of 'Let's Verify Step by Step' relate to improving trust in\n",
      "          AI systems?\n",
      "        answer: |-\n",
      "          The 'Let's Verify Step by Step' method aims to improve trust in AI systems,\n",
      "          specifically large language models, by verifying their execution step-by-step.\n",
      "          This approach allows for a more transparent and interpretable understanding of\n",
      "          the model's decision-making process, which can help build confidence in AI\n",
      "          systems.\n",
      "  - context: \"The multi-iteration and parallel-chain extensions introduced in Section\\\n",
      "      \\ 4.2.1 provides two more axes to spend computation in addition to the number\\\n",
      "      \\ of particles. We now explore how different ways to allocate budgets changes\\\n",
      "      \\ the performance. Specifically, we study for a fixed budget N \\xD7 T \\xD7 M\\\n",
      "      \\ , how the combination of N,T,M can yield the best performance, where N is\\\n",
      "      \\ the number of particles, T is the number of iterations, and M is the number\\\n",
      "      \\ of parallelism.\\nAllocating budget between N and T Figure 7 shows results\\\n",
      "      \\ of Llama-3.2 1B model when configured with various test-time compute budget\\\n",
      "      \\ allocations. Although the plot shows that various Particle Gibbs configurations\\\n",
      "      \\ do not have a marked benefit over an equivalently budgeted particle filtering\\\n",
      "      \\ run, a PG experiment with 16 particles and 4 iterations powered by a Qwen\\\n",
      "      \\ 2.5 7B Math Instruct policy model achieved a 87.2% accuracy on MATH500, beating\\\n",
      "      \\ o1 performance. Configurations with larger N values typically do better than\\\n",
      "      \\ equivalently budgeted runs with less particles.\\nFigure 8. Comparison of PF\\\n",
      "      \\ and PT with different particle group sizes, evaluated on a 100-question subset\\\n",
      "      \\ of the MATH500 dataset using Llama-3.2-1B-Instruct as the policy model.\\n\\\n",
      "      Allocating budget between N and M Figure 8 shows PF and 3 PT configurations\\\n",
      "      \\ over a set of increasing numbers of budgets. First, as we can see, for any\\\n",
      "      \\ fixed N , increasing M also improves the performance. This may be helpful\\\n",
      "      \\ when combining batch generation with distributed computing. Second, PT with\\\n",
      "      \\ N = 16 has a better overall scaling than PF. This indicates that there is\\\n",
      "      \\ some optimal budget allocation over parallel chains that can further improve\\\n",
      "      \\ the overall performance of our main results.\\nWe leave the exploration over\\\n",
      "      \\ the optimal configuration of N,T,M jointly as a future work.\"\n",
      "    questions_and_answers:\n",
      "      - question: |-\n",
      "          How does allocating budget between 'N' and 'T' affect performance according to\n",
      "          Figure 7?\n",
      "        answer: |-\n",
      "          Figure 7 shows that configurations with larger 'N' values typically perform\n",
      "          better than equivalently budgeted runs with fewer particles, despite Particle\n",
      "          Gibbs not having a marked benefit over particle filtering within the same\n",
      "          budget.\n",
      "      - question: |-\n",
      "          Given the improvements seen with increasing 'M' for a fixed 'N' in Figure 8,\n",
      "          what can be inferred about the potential benefits of allocating budget between\n",
      "          'N' and 'M'?\n",
      "        answer: |-\n",
      "          Increasing 'M' while keeping 'N' constant improves performance, suggesting that\n",
      "          allocating more budget to 'M' (parallelism) could enhance overall system\n",
      "          efficiency, especially when combined with distributed computing.\n",
      "  - context: \"Figure 9. Particle filtering for inference scaling in details. We initialize\\\n",
      "      \\ x\\nparticles with the 'first step' of an answer to a question. At every step,\\\n",
      "      \\ each\\nparticle p i is given a score s t i by the PRM, which is then used as\\\n",
      "      \\ a weight w\\nt i to determine how likely that particle is to be resampled (evolved\\\n",
      "      \\ via a\\nsolid line) at the next step. A particle is deemed 'active' (green,\\\n",
      "      \\ in this\\ndiagram) until it generates an \\u2329 EOS \\u232A token, after which\\\n",
      "      \\ it is still able to be\\nresampled (evolved via a dashed line) but is not evolved\\\n",
      "      \\ further. This process\\ncontinues until all particles have completed their\\\n",
      "      \\ answers and become inactive\\n(filled yellow).\"\n",
      "    questions_and_answers:\n",
      "      - question: What is the initial state of the particles in the particle filtering\n",
      "          process?\n",
      "        answer: The particles are initialized with the 'first step' of an answer to\n",
      "          a question.\n",
      "      - question: |-\n",
      "          How are the particles scored and used in the subsequent steps of the particle\n",
      "          filtering process?\n",
      "        answer: |-\n",
      "          Each particle is given a score by the PRM, which is then used as a weight to\n",
      "          determine its likelihood of being resampled at the next step. Active particles\n",
      "          generate an 'EOS' token before becoming inactive and still eligible for\n",
      "          resampling but not further evolution.\n",
      "      - question: |-\n",
      "          Why does the process of particle filtering eventually lead to all particles\n",
      "          becoming inactive?\n",
      "        answer: |-\n",
      "          All particles become inactive once they generate an 'EOS' token, indicating the\n",
      "          completion of their answers.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(qna_output_path) as yaml_file:\n",
    "    print(yaml_file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f101076-a50f-49ea-a83b-46eaa8b39cc4",
   "metadata": {},
   "source": [
    "# Create Seed Dataset for SDG\n",
    "\n",
    "This notebook combines the contents from the qna.yaml and the chunks from the source document to create a seed dataset for the synthetic data generation process.\n",
    "\n",
    "To run this notebook you need a directory that contains N chunks named `{original-file-name}-{N}.txt` and a `qna.yaml` in the same directory.\n",
    "\n",
    "This notebook outputs a `seed.jsonl` file in the `output_dir` that you set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2c6e31b-e8a9-406c-b2dc-27433c8fd8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install -qq datasets transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab2c9ed2-8ba8-4959-8e01-81625b81d286",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "qna.yaml file is missing document_outline, domain, or seed_examples fields",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     11\u001b[39m         shutil.copy(full_file_name, SEED_EXAMPLE_INPUT_DIR)\n\u001b[32m     13\u001b[39m shutil.copy(qna_output_path, SEED_EXAMPLE_INPUT_DIR)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m seed_data = \u001b[43mget_seed_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSEED_EXAMPLE_INPUT_DIR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m seed_data.to_json(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSEED_EXAMPLE_OUTPUT_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/seed_data.jsonl\u001b[39m\u001b[33m'\u001b[39m, orient=\u001b[33m'\u001b[39m\u001b[33mrecords\u001b[39m\u001b[33m'\u001b[39m, lines=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/code/examples/notebooks/instructlab-knowledge-pipeline/utils/create_seed_dataset.py:23\u001b[39m, in \u001b[36mget_seed_dataset\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[33;03mCreates a seed dataset from a path\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     20\u001b[39m \u001b[33;03m                  SDG.\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     22\u001b[39m valid_path = is_dir_valid(path)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m ds = \u001b[43mcreate_dataset_from_dir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalid_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/code/examples/notebooks/instructlab-knowledge-pipeline/utils/create_seed_dataset.py:95\u001b[39m, in \u001b[36mcreate_dataset_from_dir\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;66;03m# Check for required fields\u001b[39;00m\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(key \u001b[38;5;129;01min\u001b[39;00m qna_yaml \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m'\u001b[39m\u001b[33mdocument_outline\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mdomain\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mseed_examples\u001b[39m\u001b[33m'\u001b[39m]):\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mqna.yaml file is missing document_outline, domain, or seed_examples fields\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     97\u001b[39m chunks_dict = read_chunks(path)\n\u001b[32m     99\u001b[39m datasets = []\n",
      "\u001b[31mValueError\u001b[39m: qna.yaml file is missing document_outline, domain, or seed_examples fields"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "from utils.create_seed_dataset import get_seed_dataset\n",
    "\n",
    "src_files = os.listdir(CHUNKING_OUTPUT_DIR)\n",
    "\n",
    "for file_name in src_files:\n",
    "    full_file_name = os.path.join(CHUNKING_OUTPUT_DIR, file_name)\n",
    "    if os.path.isfile(full_file_name):\n",
    "        shutil.copy(full_file_name, SEED_EXAMPLE_INPUT_DIR)\n",
    "\n",
    "shutil.copy(qna_output_path, SEED_EXAMPLE_INPUT_DIR)\n",
    "\n",
    "seed_data = get_seed_dataset(SEED_EXAMPLE_INPUT_DIR)\n",
    "seed_data.to_json(f'{SEED_EXAMPLE_OUTPUT_DIR}/seed_data.jsonl', orient='records', lines=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
